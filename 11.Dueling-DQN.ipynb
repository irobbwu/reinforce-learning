{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61bb43f-d5be-497c-b302-a7a76a6e9049",
   "metadata": {},
   "source": [
    "# 11.Dueling-DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda8066d-a035-414c-9c4d-f701d6c3a60c",
   "metadata": {},
   "source": [
    "这个对DQN的改进是在2015年的“Dueling Network Architectures for Deep Reinforcement Learning”论文中提出的。\r\n",
    "\r\n",
    "该论文的核心发现是，神经网络所试图逼近的Q值Q(s, a)可以被分成两个量：状态的价值V(s)，以及这个状态下的动作优势A(s, a)。\r\n",
    "\r\n",
    "在同一个状态下，所有动作的优势值之和为，因为所有动作的动作价值的期望就是这个状态的状态价值。\r\n",
    "\r\n",
    "这种约束可以通过几种方法来实施，例如，通过损失函数。但是在论文中，作者提出一个非常巧妙的解决方案，就是从神经网络的Q表达式中减去优势值的平均值，它有效地将优势值的平均值趋于0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de3e7ba-465c-4ab4-9fce-a29a735faf20",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(s,a) = V(a)+A(s,a)-\\frac{1}{n}\\sum _{a'}A(s,a')\n",
    "$$\n",
    "\n",
    "这使得对基础DQN的改动变得很\r\n",
    "简单：为了将其转换成Dueling DQN，只需要改变神经网络的结构，而\r\n",
    "不需要影响其他部分的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6af8a5-f3db-452e-aca1-4415b6053543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gym.envs.toy_text import frozen_lake\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c325c7f-8676-4b7e-84f2-13c5dcccc175",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DuelingNet(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, q_table_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # 动作优势A(s, a)\n",
    "        self.a_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, q_table_size),\n",
    "        )\n",
    "\n",
    "        # 价值V(s)\n",
    "        self.v_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        if len(torch.Tensor(state).size())==1:\n",
    "            state = state.reshape(1,-1)\n",
    "        v = self.v_net(state)\n",
    "        a = self.a_net(state)\n",
    "        mean_a = a.mean(dim=1,keepdim=True)\n",
    "        # torch.mean(a, axis=1).reshape(-1, 1)\n",
    "        return v + a - mean_a\n",
    "\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n,)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1ec6c-2405-4b4d-be91-61577e8fc211",
   "metadata": {},
   "source": [
    "# ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2face0-8a57-4e6a-af31-5378d5cb8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, queue_size, replay_time):\n",
    "        self.queue = []\n",
    "        self.queue_size = queue_size\n",
    "        self.replay_time = replay_time\n",
    "\n",
    "    def get_batch_queue(self, env, action_trigger, batch_size, epsilon):\n",
    "        def insert_sample_to_queue(env):\n",
    "            state, info = env.reset()\n",
    "            stop = 0\n",
    "\n",
    "            while True:\n",
    "                if np.random.uniform(0, 1, 1) > epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = action_trigger(state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                self.queue.append([state, action, next_state, reward, terminated])\n",
    "                state = next_state\n",
    "                if terminated:\n",
    "                    state, info = env.reset()\n",
    "                    stop += 1\n",
    "                    continue\n",
    "                if stop >= replay_time:\n",
    "                    break\n",
    "\n",
    "        def init_queue(env):\n",
    "            while True:\n",
    "                insert_sample_to_queue(env)\n",
    "                if len(self.queue) >= self.queue_size:\n",
    "                    break\n",
    "\n",
    "        init_queue(env)\n",
    "        insert_sample_to_queue(env)\n",
    "        self.queue = self.queue[-self.queue_size :]\n",
    "\n",
    "        return random.sample(self.queue, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be83a26-89f6-4c9a-9185-1dc7203612c6",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f672a2bf-05df-418c-91a4-d661f35087b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, obs_size, hidden_size, q_table_size, net):\n",
    "        self.env = env\n",
    "        self.net = net(obs_size, hidden_size, q_table_size)\n",
    "        self.tgt_net = net(obs_size, hidden_size, q_table_size)\n",
    "\n",
    "    # 更新net参数\n",
    "    def update_net_parameters(self, update=True):\n",
    "        self.net.load_state_dict(self.tgt_net.state_dict())\n",
    "\n",
    "    def get_action_trigger(self, state):\n",
    "        state = torch.Tensor(state)\n",
    "        action = int(torch.argmax(self.tgt_net(state).detach()))\n",
    "        return action\n",
    "\n",
    "    # 计算y_hat_and_y\n",
    "    def calculate_y_hat_and_y(self, batch, gamma):\n",
    "        y = []\n",
    "        action_sapce = []\n",
    "        state_sapce = []\n",
    "\n",
    "        for state, action, next_state, reward, terminated in batch:\n",
    "            q_table_net = self.net(torch.Tensor(next_state)).detach()\n",
    "            y.append(reward + (1 - terminated) * gamma * float(torch.max(q_table_net)))\n",
    "            action_sapce.append(action)\n",
    "            state_sapce.append(state)\n",
    "        y_hat = self.tgt_net(torch.Tensor(np.array(state_sapce)))\n",
    "        y_hat = y_hat.gather(1, torch.LongTensor(action_sapce).reshape(-1, 1))\n",
    "        return y_hat.reshape(-1), torch.tensor(y)\n",
    "\n",
    "    def predict_reward(self):\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        reward_space = []\n",
    "\n",
    "        while True:\n",
    "            step += 1\n",
    "            state = torch.Tensor(state)\n",
    "            action = int(torch.argmax(self.net(state).detach()))\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            reward_space.append(reward)\n",
    "            state = next_state\n",
    "            if terminated:\n",
    "                state, info = env.reset()\n",
    "                continue\n",
    "            if step >= 100:\n",
    "                break\n",
    "        return float(np.mean(reward_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c65fde-9f1d-4f36-b79b-e06a314ad5bf",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d34d36-fe18-4790-9194-46818a4b9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "queue_size = 500\n",
    "replay_time = 50\n",
    "\n",
    "## 初始化环境\n",
    "env = frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "env.spec = gym.spec(\"FrozenLake-v1\")\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "env = DiscreteOneHotWrapper(env)\n",
    "\n",
    "## 初始化buffer\n",
    "replay_buffer = ReplayBuffer(queue_size, replay_time)\n",
    "\n",
    "## 初始化dqn\n",
    "obs_size = env.observation_space.shape[0]\n",
    "q_table_size = env.action_space.n\n",
    "dqn = DQN(env, obs_size, hidden_size, q_table_size, DuelingNet)\n",
    "\n",
    "# 定义优化器\n",
    "opt = optim.Adam(dqn.tgt_net.parameters(), lr=0.01)\n",
    "\n",
    "# 定义损失函数\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"logs/DQN/Dueling-DQN\", comment=\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd682d3-7daf-460f-a040-4093ddc6b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epsilon = 0.8\n",
    "epochs = 500\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f59cea-35d7-49f9-8296-4c203c10fef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,  MSE: 0.009532208554446697, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:1,  MSE: 0.005161388777196407, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:2,  MSE: 0.008616828359663486, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:3,  MSE: 0.006230560131371021, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:4,  MSE: 0.005877045448869467, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:5,  MSE: 0.0029788087122142315, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:6,  MSE: 0.001360040856525302, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:7,  MSE: 0.001736717065796256, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:8,  MSE: 0.0011424050899222493, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:9,  MSE: 0.0009270317386835814, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:10,  MSE: 0.0028675051871687174, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:11,  MSE: 0.0012744665145874023, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:12,  MSE: 0.00660003162920475, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:13,  MSE: 0.005725443828850985, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:14,  MSE: 0.0030118459835648537, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:15,  MSE: 0.004209105856716633, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:16,  MSE: 0.003958996385335922, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:17,  MSE: 0.0034267797600477934, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:18,  MSE: 0.002259638160467148, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:19,  MSE: 0.0017911120085045695, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:20,  MSE: 0.002288453048095107, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:21,  MSE: 0.001143297995440662, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:22,  MSE: 0.0014195936964824796, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:23,  MSE: 0.0011224265908822417, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:24,  MSE: 0.0006843149312771857, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:25,  MSE: 0.0005431441240943968, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:26,  MSE: 0.0004722446610685438, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:27,  MSE: 0.0008776565664447844, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:28,  MSE: 0.0009379392140544951, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:29,  MSE: 0.0022976610343903303, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:30,  MSE: 0.0009995673317462206, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:31,  MSE: 0.0003344804863445461, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:32,  MSE: 0.0012758738594129682, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:33,  MSE: 0.0009976534638553858, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:34,  MSE: 0.0004933643504045904, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:35,  MSE: 0.003862477606162429, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:36,  MSE: 0.00036383565748110414, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:37,  MSE: 0.000558162631932646, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:38,  MSE: 0.00043086218647658825, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:39,  MSE: 0.0036755504552274942, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:40,  MSE: 0.0003305087157059461, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:41,  MSE: 0.002584907691925764, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:42,  MSE: 0.003968560602515936, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:43,  MSE: 0.0030717363115400076, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:44,  MSE: 0.011666128411889076, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:45,  MSE: 0.0285683274269104, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:46,  MSE: 0.016642453148961067, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:47,  MSE: 0.00794562790542841, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:48,  MSE: 0.0036319762002676725, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:49,  MSE: 0.005357611924409866, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:50,  MSE: 0.010613176971673965, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:51,  MSE: 0.009360367432236671, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:52,  MSE: 0.0115344924852252, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:53,  MSE: 0.008692591451108456, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:54,  MSE: 0.006775295361876488, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:55,  MSE: 0.008272288367152214, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:56,  MSE: 0.006889946758747101, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:57,  MSE: 0.006399194244295359, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:58,  MSE: 0.006148970685899258, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:59,  MSE: 0.007120832335203886, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:60,  MSE: 0.0034653018228709698, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:61,  MSE: 0.005986259318888187, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:62,  MSE: 0.005542598199099302, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:63,  MSE: 0.004054172895848751, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:64,  MSE: 0.004920247010886669, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:65,  MSE: 0.004488223232328892, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:66,  MSE: 0.0026229091454297304, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:67,  MSE: 0.0028418460860848427, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:68,  MSE: 0.003225355874747038, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:69,  MSE: 0.0021454968955367804, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:70,  MSE: 0.0016958866035565734, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:71,  MSE: 0.001550357905216515, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:72,  MSE: 0.0020231965463608503, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:73,  MSE: 0.0008659150335006416, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:74,  MSE: 0.02577930875122547, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:75,  MSE: 0.025850191712379456, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:76,  MSE: 0.014066094532608986, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:77,  MSE: 0.009408973157405853, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:78,  MSE: 0.006838046945631504, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:79,  MSE: 0.013715317472815514, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:80,  MSE: 0.00256332173012197, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:81,  MSE: 0.005409814417362213, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:82,  MSE: 0.003527476452291012, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:83,  MSE: 0.004725323058664799, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:84,  MSE: 0.004631584510207176, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:85,  MSE: 0.004575261380523443, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:86,  MSE: 0.01277213916182518, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:87,  MSE: 0.011210472323000431, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:88,  MSE: 0.009385904297232628, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:89,  MSE: 0.010514439083635807, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:90,  MSE: 0.008150768466293812, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:91,  MSE: 0.028147205710411072, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:92,  MSE: 0.021140191704034805, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:93,  MSE: 0.013324248604476452, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:94,  MSE: 0.03463765233755112, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:95,  MSE: 0.027985744178295135, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:96,  MSE: 0.02026250585913658, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:97,  MSE: 0.018960662186145782, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:98,  MSE: 0.04710108041763306, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:99,  MSE: 0.01941785216331482, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:100,  MSE: 0.012866195291280746, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:101,  MSE: 0.031077764928340912, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:102,  MSE: 0.039534565061330795, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:103,  MSE: 0.05907125025987625, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:104,  MSE: 0.038612037897109985, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:105,  MSE: 0.015137545764446259, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:106,  MSE: 0.013418955728411674, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:107,  MSE: 0.013521330431103706, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:108,  MSE: 0.01367373950779438, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:109,  MSE: 0.012611663900315762, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:110,  MSE: 0.006523147691041231, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:111,  MSE: 0.008265373297035694, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:112,  MSE: 0.0075182318687438965, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:113,  MSE: 0.006255594547837973, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:114,  MSE: 0.005352974869310856, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:115,  MSE: 0.007668584119528532, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:116,  MSE: 0.0059917825274169445, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:117,  MSE: 0.004889334086328745, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:118,  MSE: 0.006648609414696693, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:119,  MSE: 0.00805667508393526, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:120,  MSE: 0.007870668545365334, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:121,  MSE: 0.010714644566178322, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:122,  MSE: 0.007583183236420155, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:123,  MSE: 0.004840938374400139, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:124,  MSE: 0.004061422310769558, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:125,  MSE: 0.004203157965093851, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:126,  MSE: 0.003042879281565547, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:127,  MSE: 0.00680199358612299, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:128,  MSE: 0.0037755388766527176, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:129,  MSE: 0.0013602707767859101, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:130,  MSE: 0.011372782289981842, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:131,  MSE: 0.00776669243350625, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:132,  MSE: 0.005172520875930786, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:133,  MSE: 0.004500790499150753, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:134,  MSE: 0.008071156218647957, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:135,  MSE: 0.0065581537783145905, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:136,  MSE: 0.005865956656634808, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:137,  MSE: 0.044403303414583206, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:138,  MSE: 0.05225687846541405, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:139,  MSE: 0.031142905354499817, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:140,  MSE: 0.006439149845391512, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:141,  MSE: 0.004500014707446098, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:142,  MSE: 0.004875471815466881, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:143,  MSE: 0.0033656242303550243, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:144,  MSE: 0.0058379992842674255, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:145,  MSE: 0.005497119389474392, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:146,  MSE: 0.012524711899459362, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:147,  MSE: 0.011592632159590721, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:148,  MSE: 0.00862716045230627, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:149,  MSE: 0.009235733188688755, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:150,  MSE: 0.00863019097596407, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:151,  MSE: 0.013311618939042091, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:152,  MSE: 0.024845732375979424, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:153,  MSE: 0.009743290022015572, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:154,  MSE: 0.028442980721592903, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:155,  MSE: 0.032438863068819046, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:156,  MSE: 0.014628861099481583, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:157,  MSE: 0.012609578669071198, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:158,  MSE: 0.042298007756471634, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:159,  MSE: 0.036671143025159836, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:160,  MSE: 0.03789466992020607, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:161,  MSE: 0.06425163149833679, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:162,  MSE: 0.05427546799182892, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:163,  MSE: 0.04944394528865814, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:164,  MSE: 0.041267361491918564, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:165,  MSE: 0.027414627373218536, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:166,  MSE: 0.015113115310668945, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:167,  MSE: 0.009455311112105846, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:168,  MSE: 0.08931262791156769, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:169,  MSE: 0.08502127230167389, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:170,  MSE: 0.05181621015071869, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:171,  MSE: 0.08545537292957306, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:172,  MSE: 0.08023373037576675, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:173,  MSE: 0.0708082988858223, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:174,  MSE: 0.0635116696357727, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:175,  MSE: 0.046290554106235504, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:176,  MSE: 0.044338688254356384, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:177,  MSE: 0.01704241894185543, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:178,  MSE: 0.01666533201932907, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:179,  MSE: 0.016271241009235382, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:180,  MSE: 0.013002293184399605, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:181,  MSE: 0.09399720281362534, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:182,  MSE: 0.0842747837305069, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:183,  MSE: 0.08803893625736237, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:184,  MSE: 0.08213954418897629, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:185,  MSE: 0.07040713727474213, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:186,  MSE: 0.023012544959783554, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:187,  MSE: 0.021139396354556084, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:188,  MSE: 0.011202432215213776, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:189,  MSE: 0.015391330234706402, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:190,  MSE: 0.01674596220254898, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:191,  MSE: 0.016074268147349358, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:192,  MSE: 0.0153272096067667, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:193,  MSE: 0.01036886963993311, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:194,  MSE: 0.012628495693206787, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:195,  MSE: 0.015416635200381279, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:196,  MSE: 0.010470874607563019, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:197,  MSE: 0.006052692420780659, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:198,  MSE: 0.005282742902636528, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:199,  MSE: 0.007925721816718578, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:200,  MSE: 0.007797700352966785, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:201,  MSE: 0.012250416912138462, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:202,  MSE: 0.01368413120508194, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:203,  MSE: 0.00757480226457119, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:204,  MSE: 0.007539179641753435, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:205,  MSE: 0.010367898270487785, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:206,  MSE: 0.006856063846498728, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:207,  MSE: 0.00790175050497055, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:208,  MSE: 0.014572590589523315, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:209,  MSE: 0.014591345563530922, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:210,  MSE: 0.01187786553055048, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:211,  MSE: 0.015140331350266933, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:212,  MSE: 0.011716309003531933, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:213,  MSE: 0.013498165644705296, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:214,  MSE: 0.011086140759289265, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:215,  MSE: 0.007721837144345045, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:216,  MSE: 0.005443126428872347, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:217,  MSE: 0.0033065034076571465, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:218,  MSE: 0.00452851876616478, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:219,  MSE: 0.004320631735026836, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:220,  MSE: 0.0045786392875015736, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:221,  MSE: 0.019826197996735573, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:222,  MSE: 0.01752226985991001, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:223,  MSE: 0.01636749878525734, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:224,  MSE: 0.013749188743531704, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:225,  MSE: 0.013700125738978386, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:226,  MSE: 0.010866100899875164, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:227,  MSE: 0.00666519720107317, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:228,  MSE: 0.006913397926837206, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:229,  MSE: 0.008536330424249172, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:230,  MSE: 0.006977166049182415, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:231,  MSE: 0.04147383198142052, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:232,  MSE: 0.034832440316677094, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:233,  MSE: 0.027046816423535347, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:234,  MSE: 0.02149827964603901, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:235,  MSE: 0.01630302518606186, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:236,  MSE: 0.013817433267831802, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:237,  MSE: 0.010077585466206074, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:238,  MSE: 0.010767357423901558, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:239,  MSE: 0.012456271797418594, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:240,  MSE: 0.005202825181186199, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:241,  MSE: 0.055929381400346756, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:242,  MSE: 0.04333050921559334, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:243,  MSE: 0.027657585218548775, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:244,  MSE: 0.021011626347899437, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:245,  MSE: 0.017210669815540314, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:246,  MSE: 0.047140657901763916, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:247,  MSE: 0.036782462149858475, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:248,  MSE: 0.018146095797419548, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:249,  MSE: 0.0072314199060201645, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:250,  MSE: 0.010352885350584984, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:251,  MSE: 0.01529743243008852, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:252,  MSE: 0.028301209211349487, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:253,  MSE: 0.03368210792541504, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:254,  MSE: 0.023655476048588753, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:255,  MSE: 0.017928192391991615, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:256,  MSE: 0.01990869641304016, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:257,  MSE: 0.018928609788417816, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:258,  MSE: 0.029638292267918587, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:259,  MSE: 0.01350446417927742, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:260,  MSE: 0.030588459223508835, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:261,  MSE: 0.010825542733073235, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:262,  MSE: 0.009946929290890694, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:263,  MSE: 0.008470023982226849, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:264,  MSE: 0.005810062400996685, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:265,  MSE: 0.003983521368354559, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:266,  MSE: 0.005268111824989319, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:267,  MSE: 0.0065673887729644775, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:268,  MSE: 0.006336086895316839, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:269,  MSE: 0.00447228504344821, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:270,  MSE: 0.003321667667478323, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:271,  MSE: 0.00690420949831605, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:272,  MSE: 0.004928708076477051, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:273,  MSE: 0.0028352525550872087, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:274,  MSE: 0.00267833867110312, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:275,  MSE: 0.0038532977923750877, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:276,  MSE: 0.003965114243328571, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:277,  MSE: 0.003985344432294369, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:278,  MSE: 0.00275706360116601, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:279,  MSE: 0.0023922764230519533, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:280,  MSE: 0.002497151494026184, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:281,  MSE: 0.006869324948638678, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:282,  MSE: 0.0055169048719108105, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:283,  MSE: 0.002869686344638467, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:284,  MSE: 0.0016626478172838688, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:285,  MSE: 0.0019132125889882445, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:286,  MSE: 0.0025996509939432144, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:287,  MSE: 0.00227216724306345, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:288,  MSE: 0.00226596905849874, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:289,  MSE: 0.0019171256572008133, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:290,  MSE: 0.0009152947459369898, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:291,  MSE: 0.004338342696428299, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:292,  MSE: 0.003221391700208187, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:293,  MSE: 0.0016464886721223593, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:294,  MSE: 0.0013885730877518654, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:295,  MSE: 0.0021552667021751404, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:296,  MSE: 0.0023362720385193825, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:297,  MSE: 0.0023226120974868536, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:298,  MSE: 0.003092842409387231, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:299,  MSE: 0.0033863529097288847, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:300,  MSE: 0.0021658940240740776, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:301,  MSE: 0.0038411954883486032, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:302,  MSE: 0.003949662670493126, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:303,  MSE: 0.0029150759801268578, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:304,  MSE: 0.0013942231889814138, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:305,  MSE: 0.0016588232247158885, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:306,  MSE: 0.001142654218710959, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:307,  MSE: 0.0014655336271971464, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:308,  MSE: 0.0008520948467776179, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:309,  MSE: 0.0019613325130194426, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:310,  MSE: 0.001149098970927298, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:311,  MSE: 0.001992657547816634, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:312,  MSE: 0.0025130142457783222, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:313,  MSE: 0.0009988188976421952, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:314,  MSE: 0.003005807986482978, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:315,  MSE: 0.0018720818916335702, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:316,  MSE: 0.002577034989371896, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:317,  MSE: 0.004646360874176025, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:318,  MSE: 0.003428144147619605, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:319,  MSE: 0.0023077467922121286, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:320,  MSE: 0.0036074472591280937, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:321,  MSE: 0.0054488275200128555, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:322,  MSE: 0.003719691652804613, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:323,  MSE: 0.0070402007550001144, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:324,  MSE: 0.007066721096634865, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:325,  MSE: 0.004733291454613209, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:326,  MSE: 0.005778820253908634, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:327,  MSE: 0.004803856834769249, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:328,  MSE: 0.00460992194712162, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:329,  MSE: 0.0048865340650081635, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:330,  MSE: 0.002527872333303094, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:331,  MSE: 0.0034539939370006323, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:332,  MSE: 0.0025913193821907043, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:333,  MSE: 0.002781652146950364, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:334,  MSE: 0.003135072533041239, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:335,  MSE: 0.0019622144754976034, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:336,  MSE: 0.0025249335449188948, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:337,  MSE: 0.002268137875944376, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:338,  MSE: 0.001076463726349175, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:339,  MSE: 0.003062875010073185, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:340,  MSE: 0.0030376226641237736, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:341,  MSE: 0.005788471084088087, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:342,  MSE: 0.004345663823187351, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:343,  MSE: 0.0046709319576621056, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:344,  MSE: 0.004093542695045471, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:345,  MSE: 0.004013990052044392, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:346,  MSE: 0.003964963369071484, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:347,  MSE: 0.00395953468978405, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:348,  MSE: 0.003384360345080495, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:349,  MSE: 0.0035055263433605433, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:350,  MSE: 0.0025696544907987118, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:351,  MSE: 0.005078114103525877, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:352,  MSE: 0.00429012393578887, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:353,  MSE: 0.003773194272071123, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:354,  MSE: 0.003000388154760003, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:355,  MSE: 0.0025768454652279615, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:356,  MSE: 0.002684546634554863, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:357,  MSE: 0.002507640980184078, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:358,  MSE: 0.00211799587123096, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:359,  MSE: 0.0017061748076230288, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:360,  MSE: 0.0016760213766247034, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:361,  MSE: 0.00839410349726677, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:362,  MSE: 0.006822844967246056, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:363,  MSE: 0.005204914137721062, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:364,  MSE: 0.003476243931800127, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:365,  MSE: 0.0022780420258641243, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:366,  MSE: 0.00251030083745718, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:367,  MSE: 0.0030015145894140005, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:368,  MSE: 0.0031497469171881676, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:369,  MSE: 0.003281199373304844, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:370,  MSE: 0.0018415831727907062, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:371,  MSE: 0.011514062061905861, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:372,  MSE: 0.008600372821092606, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:373,  MSE: 0.007474331650882959, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:374,  MSE: 0.005853547714650631, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:375,  MSE: 0.005688004195690155, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:376,  MSE: 0.004777122288942337, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:377,  MSE: 0.0034902070183306932, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:378,  MSE: 0.0031291861087083817, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:379,  MSE: 0.0021104884799569845, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:380,  MSE: 0.0021526303607970476, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:381,  MSE: 0.00888573843985796, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:382,  MSE: 0.007408537901937962, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:383,  MSE: 0.00906720943748951, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:384,  MSE: 0.006915045436471701, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:385,  MSE: 0.0074257757514715195, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:386,  MSE: 0.00498105026781559, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:387,  MSE: 0.007087785750627518, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:388,  MSE: 0.006091348826885223, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:389,  MSE: 0.004161024931818247, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:390,  MSE: 0.004676853772252798, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:391,  MSE: 0.006230000406503677, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:392,  MSE: 0.00609955657273531, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:393,  MSE: 0.00487211998552084, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:394,  MSE: 0.004708630498498678, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:395,  MSE: 0.0036378754302859306, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:396,  MSE: 0.008925756439566612, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:397,  MSE: 0.0021335408091545105, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:398,  MSE: 0.003083666553720832, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:399,  MSE: 0.0029878299683332443, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:400,  MSE: 0.002437138231471181, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:401,  MSE: 0.01224982924759388, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:402,  MSE: 0.0116869006305933, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:403,  MSE: 0.010301374830305576, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:404,  MSE: 0.008936461992561817, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:405,  MSE: 0.00793375913053751, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:406,  MSE: 0.008002606220543385, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:407,  MSE: 0.006265738047659397, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:408,  MSE: 0.00502767413854599, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:409,  MSE: 0.004837743006646633, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:410,  MSE: 0.004490428138524294, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:411,  MSE: 0.0106697678565979, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:412,  MSE: 0.011761408299207687, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:413,  MSE: 0.008720778860151768, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:414,  MSE: 0.007037530653178692, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:415,  MSE: 0.0028949722182005644, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:416,  MSE: 0.004201598931103945, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:417,  MSE: 0.0037252071779221296, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:418,  MSE: 0.004244915209710598, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:419,  MSE: 0.0034656936768442392, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:420,  MSE: 0.0028644518461078405, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:421,  MSE: 0.012281596660614014, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:422,  MSE: 0.013967732898890972, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:423,  MSE: 0.012450328096747398, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:424,  MSE: 0.011136382818222046, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:425,  MSE: 0.00918365828692913, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:426,  MSE: 0.008311226963996887, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:427,  MSE: 0.007286700885742903, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:428,  MSE: 0.005014106631278992, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:429,  MSE: 0.0053223855793476105, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:430,  MSE: 0.004267098382115364, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:431,  MSE: 0.007332228124141693, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:432,  MSE: 0.00549262436106801, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:433,  MSE: 0.005170144140720367, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:434,  MSE: 0.01034767460078001, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:435,  MSE: 0.01242533978074789, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:436,  MSE: 0.007687690202146769, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:437,  MSE: 0.00789517443627119, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:438,  MSE: 0.008025875315070152, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:439,  MSE: 0.005946388468146324, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:440,  MSE: 0.005213793367147446, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:441,  MSE: 0.003583944169804454, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:442,  MSE: 0.003292599692940712, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:443,  MSE: 0.0030289082787930965, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:444,  MSE: 0.003031737171113491, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:445,  MSE: 0.0026958303060382605, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:446,  MSE: 0.0023580004926770926, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:447,  MSE: 0.00223070802167058, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:448,  MSE: 0.001950357691384852, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:449,  MSE: 0.0019426231738179922, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:450,  MSE: 0.0019148556748405099, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:451,  MSE: 0.0066627333872020245, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:452,  MSE: 0.006777038797736168, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:453,  MSE: 0.005158973392099142, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:454,  MSE: 0.004171920008957386, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:455,  MSE: 0.0030891920905560255, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:456,  MSE: 0.0025764612946659327, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:457,  MSE: 0.0028805495239794254, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:458,  MSE: 0.0031380660366266966, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:459,  MSE: 0.0032705748453736305, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:460,  MSE: 0.0034784742165356874, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:461,  MSE: 0.013801701366901398, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:462,  MSE: 0.011037010699510574, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:463,  MSE: 0.008651440031826496, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:464,  MSE: 0.00723329558968544, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:465,  MSE: 0.005380476359277964, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:466,  MSE: 0.0042357128113508224, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:467,  MSE: 0.003772041294723749, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:468,  MSE: 0.0027747810818254948, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:469,  MSE: 0.0018990618409588933, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:470,  MSE: 0.004815115127712488, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:471,  MSE: 0.004893640987575054, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:472,  MSE: 0.00454820366576314, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:473,  MSE: 0.003616275964304805, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:474,  MSE: 0.003226396394893527, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:475,  MSE: 0.002998852636665106, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:476,  MSE: 0.0023716427385807037, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:477,  MSE: 0.002358588855713606, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:478,  MSE: 0.002339711179956794, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:479,  MSE: 0.0025921056512743235, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:480,  MSE: 0.0029114841017872095, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:481,  MSE: 0.005993414670228958, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:482,  MSE: 0.007109229452908039, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:483,  MSE: 0.0045920745469629765, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:484,  MSE: 0.005882875062525272, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:485,  MSE: 0.0054383003152906895, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:486,  MSE: 0.005242578685283661, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:487,  MSE: 0.005282203666865826, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:488,  MSE: 0.004586904309689999, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:489,  MSE: 0.0023101726546883583, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:490,  MSE: 0.003128476208075881, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:491,  MSE: 0.005813894793391228, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:492,  MSE: 0.0049749636091291904, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:493,  MSE: 0.004910510033369064, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:494,  MSE: 0.0037837920244783163, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:495,  MSE: 0.0038057053461670876, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:496,  MSE: 0.00272856536321342, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:497,  MSE: 0.0020059715025126934, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:498,  MSE: 0.015232482925057411, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:499,  MSE: 0.014166295528411865, epsilon: 0.8, 100 steps reward: 0.16\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    batch = replay_buffer.get_batch_queue(\n",
    "        env, dqn.get_action_trigger, batch_size, epsilon\n",
    "    )\n",
    "    y_hat, y = dqn.calculate_y_hat_and_y(batch, gamma)\n",
    "    l = loss(y_hat, y)\n",
    "\n",
    "    # 反向传播\n",
    "    opt.zero_grad()\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        dqn.update_net_parameters()\n",
    "\n",
    "    predict_reward = dqn.predict_reward()\n",
    "    writer.add_scalars(\n",
    "        \"MSE\", {\"loss\": l.item(), \"predict_reward\": predict_reward}, epoch\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"epoch:{},  MSE: {}, epsilon: {}, 100 steps reward: {}\".format(\n",
    "            epoch, l, epsilon, predict_reward\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cccd2e9-a413-4982-b071-68aa3690d897",
   "metadata": {},
   "source": [
    "# 可视化预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82486b91-4d0f-44bd-acbc-46bc9668efcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# DQN_Q = dqn.net\n",
    "\n",
    "# env = frozen_lake.FrozenLakeEnv(is_slippery=False, render_mode=\"human\")\n",
    "# env.spec = gym.spec(\"FrozenLake-v1\")\n",
    "# # display_size = 512\n",
    "# # env.window_size = (display_size, display_size)\n",
    "# # env.cell_size = (\n",
    "# #     env.window_size[0] // env.ncol,\n",
    "# #     env.window_size[1] // env.nrow,\n",
    "# # )\n",
    "# env = gym.wrappers.RecordVideo(env, video_folder=\"video\")\n",
    "\n",
    "# env = DiscreteOneHotWrapper(env)\n",
    "\n",
    "# state, info = env.reset()\n",
    "# total_rewards = 0\n",
    "\n",
    "# while True:\n",
    "#     action = int(torch.argmax(DQN_Q(torch.Tensor(state))))\n",
    "#     state, reward, terminated, truncted, info = env.step(action)\n",
    "#     print(action)\n",
    "#     if terminated:\n",
    "#         break\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
