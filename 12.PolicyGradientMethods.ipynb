{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6e8d1e-27f3-4cfe-bd77-e18c5b6b22f6",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods\n",
    "\n",
    "## Average state value\n",
    "对所有状态值的加权平均\n",
    "$$\n",
    "\\begin {align*}\n",
    "\\bar v =& \\sum_{s\\in S}v_\\pi (s)\\\\\n",
    "=& E_{S-d}(v_\\pi(S))\\\\\n",
    "=& E_{S-d}(\\sum_{a\\in A} q(s,a)\\pi(a|s))\n",
    "\\end {align*}\n",
    "$$\n",
    "\n",
    "策略函数为$\\pi(a|s,\\theta)$\n",
    "令$J(\\theta)=\\bar v$\n",
    "对其求梯度\n",
    "\n",
    "$$\n",
    "\\begin {align*}\n",
    "\\nabla_\\theta J(\\theta) =& E_{S-d}(\\sum_{a\\in A} q(s,a)\\nabla_\\theta\\pi(a|s,\\theta))\\\\\n",
    "=& E_{S-d}(\\sum_{a\\in A} q(s,a)\\pi(a|s,\\theta) \\nabla_\\theta ln\\pi(a|s,\\theta))\\\\\n",
    "=& E_{S-d}[E_{a-\\pi(S,\\Theta)}[q(s,a) \\nabla_\\theta ln\\pi(a|s,\\theta)]]\\\\\n",
    "=& E_{S-d,a-\\pi(S,\\Theta)}[q(s,a) \\nabla_\\theta ln\\pi(a|s,\\theta)]\n",
    "\\end {align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518c18b-a500-4430-8050-a9a45e74bb7b",
   "metadata": {},
   "source": [
    "## Average reward\n",
    "$$\\bar r = (1-\\gamma)\\bar v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cbd39d-55b9-4ddf-9813-67b697fcd670",
   "metadata": {},
   "source": [
    "## Monte Carlo policy gradient (REINFORCE)\n",
    "- 1、用随机权重初始化策略网络\n",
    "- 2、运行N个完整的片段，保存其(s,a,r,s')状态转移\n",
    "- 3、对于每个片段k的每一步t，计算后续步的带折扣的总奖励$Q_{k,t}=\\sum_{i=0}\\gamma_ir_i$\n",
    "- 4、计算所有状态转移的损失函数 $L=-\\sum_{k,t}Q_{k,t}ln\\pi(a_{k,t}|s_{k,t})$\n",
    "- 5、执行SGD更新权重，以最小化损失\n",
    "- 6、从步骤2开始重复，直到收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce6acc7-0240-4ce8-a222-6079032fe40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gym.envs.toy_text import frozen_lake\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ea3371-a7df-4f29-b31e-63f6a368e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、用随机权重初始化策略网络\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, obs_n, hidden_num, act_n):\n",
    "        super().__init__()\n",
    "        # 动作优势A(s, a)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_n, hidden_num),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_num, act_n),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        if len(torch.Tensor(state).size()) == 1:\n",
    "            state = state.reshape(1, -1)\n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d513a12-e0c1-4c2c-87fa-7444e781b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(R, gamma):\n",
    "    # r 为历史得分\n",
    "    n = len(R)\n",
    "    dr = 0\n",
    "    for i in range(n):\n",
    "        dr += gamma**i * R[i]\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88f07dc-eac3-4ca7-9b46-429290fe1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 2、运行N个完整的片段，保存其(s,a,r,s')状态转移\n",
    "def generate_episode(env, n_steps, net, predict=False):\n",
    "    episode_history = dict()\n",
    "    r_list = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        episode = []\n",
    "        predict_reward = []\n",
    "        state, info = env.reset()\n",
    "        while True:\n",
    "            p = net(torch.Tensor(state)).detach().numpy().reshape(-1)\n",
    "            action = np.random.choice(list(range(env.action_space.n)), p=p)\n",
    "            next_state, reward, terminated, truncted, info = env.step(action)\n",
    "            episode.append([state, action, next_state, reward, terminated])\n",
    "            predict_reward.append(reward)\n",
    "            state = next_state\n",
    "            if terminated or truncted:\n",
    "                episode_history[_] = episode\n",
    "                r_list.append(len(episode))\n",
    "                episode = []\n",
    "                predict_reward = []\n",
    "                break\n",
    "    if predict:\n",
    "        return np.mean(r_list)\n",
    "    return episode_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514034c6-ebfe-4f68-b5da-c8e12846007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于每个片段k的每一步t，计算后续步的带折扣的总奖励\n",
    "def calculate_t_discount_reward(reward_list, gamma):\n",
    "    discount_reward = []\n",
    "    total_reward = 0\n",
    "    for i in reward_list[::-1]:\n",
    "        total_reward = total_reward * gamma + i\n",
    "        discount_reward.append(total_reward)\n",
    "    return discount_reward[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8032a363-8086-444c-b1c9-e64bf0028d07",
   "metadata": {},
   "source": [
    "- 4、计算所有状态转移的损失函数 $L=-\\sum_{k,t}Q_{k,t}ln\\pi(a_{k,t}|s_{k,t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08dbad5b-e674-4faa-b767-75f7b98e3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(batch, gamma):\n",
    "    l = 0\n",
    "    for episode in batch.values():\n",
    "        reward_list = [\n",
    "            reward for state, action, next_state, reward, terminated in episode\n",
    "        ]\n",
    "        state = [state for state, action, next_state, reward, terminated in episode]\n",
    "        action = [action for state, action, next_state, reward, terminated in episode]\n",
    "        qt = calculate_t_discount_reward(reward_list, gamma)\n",
    "        pi = net(torch.Tensor(state))\n",
    "        pi_a = pi.gather(dim=1, index=torch.LongTensor(action).reshape(-1, 1))\n",
    "        l -= torch.Tensor(qt) @ torch.log(pi_a)\n",
    "    return l / len(batch.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945a8ab-4530-4126-8e42-29b33510bec4",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9802e9-7ac7-40b9-b486-54bfbca4f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 初始化环境\n",
    "env = gym.make(\"CartPole-v1\", max_episode_steps=200)\n",
    "# env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "state, info = env.reset()\n",
    "\n",
    "obs_n = env.observation_space.shape[0]\n",
    "hidden_num = 64\n",
    "act_n = env.action_space.n\n",
    "net = PolicyNet(obs_n, hidden_num, act_n)\n",
    "\n",
    "# 定义优化器\n",
    "opt = optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "# 记录\n",
    "writer = SummaryWriter(log_dir=\"logs/PolicyGradient/reinforce\", comment=\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a139fd67-3953-4e97-aec1-b392bcda6311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wu.zhengzhen\\AppData\\Local\\Temp\\ipykernel_47500\\3686437849.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  pi = net(torch.Tensor(state))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,  Loss: tensor([78.3558]), max_steps: 24.5\n",
      "epoch:1,  Loss: tensor([94.8375]), max_steps: 29.7\n",
      "epoch:2,  Loss: tensor([89.3743]), max_steps: 22.2\n",
      "epoch:3,  Loss: tensor([58.2732]), max_steps: 34.4\n",
      "epoch:4,  Loss: tensor([91.8468]), max_steps: 49.6\n",
      "epoch:5,  Loss: tensor([121.1975]), max_steps: 45.1\n",
      "epoch:6,  Loss: tensor([125.6664]), max_steps: 45.5\n",
      "epoch:7,  Loss: tensor([79.8753]), max_steps: 50.0\n",
      "epoch:8,  Loss: tensor([109.9683]), max_steps: 74.9\n",
      "epoch:9,  Loss: tensor([89.4750]), max_steps: 81.1\n",
      "epoch:10,  Loss: tensor([119.8851]), max_steps: 70.2\n",
      "epoch:11,  Loss: tensor([94.1659]), max_steps: 86.1\n",
      "epoch:12,  Loss: tensor([84.3775]), max_steps: 62.7\n",
      "epoch:13,  Loss: tensor([70.3001]), max_steps: 51.2\n",
      "epoch:14,  Loss: tensor([45.8444]), max_steps: 48.7\n",
      "epoch:15,  Loss: tensor([47.5268]), max_steps: 37.9\n",
      "epoch:16,  Loss: tensor([47.2998]), max_steps: 37.2\n",
      "epoch:17,  Loss: tensor([46.3998]), max_steps: 33.2\n",
      "epoch:18,  Loss: tensor([35.2964]), max_steps: 32.2\n",
      "epoch:19,  Loss: tensor([31.5766]), max_steps: 32.4\n",
      "epoch:20,  Loss: tensor([25.3191]), max_steps: 35.9\n",
      "epoch:21,  Loss: tensor([26.4677]), max_steps: 37.5\n",
      "epoch:22,  Loss: tensor([33.3615]), max_steps: 36.4\n",
      "epoch:23,  Loss: tensor([33.1891]), max_steps: 42.4\n",
      "epoch:24,  Loss: tensor([38.2712]), max_steps: 47.7\n",
      "epoch:25,  Loss: tensor([46.2044]), max_steps: 92.1\n",
      "epoch:26,  Loss: tensor([105.0009]), max_steps: 96.0\n",
      "epoch:27,  Loss: tensor([91.3189]), max_steps: 109.4\n",
      "epoch:28,  Loss: tensor([117.3946]), max_steps: 109.8\n",
      "epoch:29,  Loss: tensor([111.3877]), max_steps: 112.5\n",
      "epoch:30,  Loss: tensor([95.9467]), max_steps: 111.2\n",
      "epoch:31,  Loss: tensor([97.7512]), max_steps: 116.3\n",
      "epoch:32,  Loss: tensor([81.5369]), max_steps: 121.4\n",
      "epoch:33,  Loss: tensor([95.2084]), max_steps: 135.7\n",
      "epoch:34,  Loss: tensor([77.2207]), max_steps: 142.2\n",
      "epoch:35,  Loss: tensor([93.0984]), max_steps: 177.3\n",
      "epoch:36,  Loss: tensor([124.3264]), max_steps: 170.9\n",
      "epoch:37,  Loss: tensor([89.2655]), max_steps: 166.2\n",
      "epoch:38,  Loss: tensor([89.2658]), max_steps: 141.5\n",
      "epoch:39,  Loss: tensor([94.4763]), max_steps: 98.5\n",
      "epoch:40,  Loss: tensor([54.3605]), max_steps: 92.6\n",
      "epoch:41,  Loss: tensor([41.1204]), max_steps: 78.7\n",
      "epoch:42,  Loss: tensor([34.4525]), max_steps: 62.4\n",
      "epoch:43,  Loss: tensor([38.2389]), max_steps: 56.1\n",
      "epoch:44,  Loss: tensor([27.7286]), max_steps: 60.2\n",
      "epoch:45,  Loss: tensor([22.1154]), max_steps: 47.8\n",
      "epoch:46,  Loss: tensor([19.1377]), max_steps: 53.2\n",
      "epoch:47,  Loss: tensor([17.6894]), max_steps: 48.2\n",
      "epoch:48,  Loss: tensor([21.6472]), max_steps: 49.7\n",
      "epoch:49,  Loss: tensor([20.8734]), max_steps: 54.3\n",
      "epoch:50,  Loss: tensor([19.9867]), max_steps: 48.1\n",
      "epoch:51,  Loss: tensor([20.9545]), max_steps: 44.3\n",
      "epoch:52,  Loss: tensor([16.5963]), max_steps: 43.6\n",
      "epoch:53,  Loss: tensor([19.8069]), max_steps: 47.4\n",
      "epoch:54,  Loss: tensor([13.9796]), max_steps: 42.5\n",
      "epoch:55,  Loss: tensor([21.7415]), max_steps: 43.5\n",
      "epoch:56,  Loss: tensor([19.0390]), max_steps: 38.0\n",
      "epoch:57,  Loss: tensor([12.6760]), max_steps: 42.5\n",
      "epoch:58,  Loss: tensor([16.7254]), max_steps: 40.0\n",
      "epoch:59,  Loss: tensor([18.7327]), max_steps: 40.5\n",
      "epoch:60,  Loss: tensor([22.6051]), max_steps: 37.8\n",
      "epoch:61,  Loss: tensor([13.0269]), max_steps: 36.1\n",
      "epoch:62,  Loss: tensor([14.2230]), max_steps: 34.5\n",
      "epoch:63,  Loss: tensor([15.3844]), max_steps: 34.8\n",
      "epoch:64,  Loss: tensor([15.0444]), max_steps: 33.1\n",
      "epoch:65,  Loss: tensor([22.9690]), max_steps: 32.1\n",
      "epoch:66,  Loss: tensor([18.0715]), max_steps: 29.7\n",
      "epoch:67,  Loss: tensor([17.1091]), max_steps: 30.6\n",
      "epoch:68,  Loss: tensor([10.4019]), max_steps: 28.9\n",
      "epoch:69,  Loss: tensor([11.8673]), max_steps: 28.1\n",
      "epoch:70,  Loss: tensor([10.6382]), max_steps: 27.7\n",
      "epoch:71,  Loss: tensor([9.3974]), max_steps: 24.3\n",
      "epoch:72,  Loss: tensor([10.1737]), max_steps: 25.0\n",
      "epoch:73,  Loss: tensor([7.1530]), max_steps: 25.6\n",
      "epoch:74,  Loss: tensor([10.0896]), max_steps: 25.2\n",
      "epoch:75,  Loss: tensor([6.9605]), max_steps: 25.2\n",
      "epoch:76,  Loss: tensor([14.9475]), max_steps: 23.1\n",
      "epoch:77,  Loss: tensor([10.8724]), max_steps: 23.5\n",
      "epoch:78,  Loss: tensor([11.9182]), max_steps: 23.0\n",
      "epoch:79,  Loss: tensor([6.8975]), max_steps: 22.5\n",
      "epoch:80,  Loss: tensor([9.6391]), max_steps: 23.9\n",
      "epoch:81,  Loss: tensor([6.2065]), max_steps: 25.3\n",
      "epoch:82,  Loss: tensor([7.2165]), max_steps: 22.9\n",
      "epoch:83,  Loss: tensor([8.6542]), max_steps: 25.2\n",
      "epoch:84,  Loss: tensor([8.4031]), max_steps: 24.4\n",
      "epoch:85,  Loss: tensor([8.5080]), max_steps: 26.2\n",
      "epoch:86,  Loss: tensor([6.5245]), max_steps: 25.9\n",
      "epoch:87,  Loss: tensor([8.4216]), max_steps: 24.7\n",
      "epoch:88,  Loss: tensor([8.8222]), max_steps: 24.8\n",
      "epoch:89,  Loss: tensor([5.4478]), max_steps: 27.1\n",
      "epoch:90,  Loss: tensor([12.1782]), max_steps: 25.9\n",
      "epoch:91,  Loss: tensor([8.8020]), max_steps: 29.0\n",
      "epoch:92,  Loss: tensor([13.7430]), max_steps: 32.8\n",
      "epoch:93,  Loss: tensor([18.6477]), max_steps: 29.7\n",
      "epoch:94,  Loss: tensor([10.5635]), max_steps: 29.0\n",
      "epoch:95,  Loss: tensor([14.5106]), max_steps: 32.9\n",
      "epoch:96,  Loss: tensor([12.5390]), max_steps: 31.6\n",
      "epoch:97,  Loss: tensor([13.2790]), max_steps: 30.8\n",
      "epoch:98,  Loss: tensor([14.8771]), max_steps: 32.6\n",
      "epoch:99,  Loss: tensor([17.4435]), max_steps: 34.1\n",
      "epoch:100,  Loss: tensor([21.4662]), max_steps: 36.1\n",
      "epoch:101,  Loss: tensor([14.5001]), max_steps: 33.0\n",
      "epoch:102,  Loss: tensor([17.5304]), max_steps: 38.2\n",
      "epoch:103,  Loss: tensor([17.4940]), max_steps: 39.6\n",
      "epoch:104,  Loss: tensor([18.4211]), max_steps: 37.2\n",
      "epoch:105,  Loss: tensor([21.6570]), max_steps: 38.5\n",
      "epoch:106,  Loss: tensor([18.8480]), max_steps: 37.0\n",
      "epoch:107,  Loss: tensor([20.7468]), max_steps: 38.1\n",
      "epoch:108,  Loss: tensor([19.8942]), max_steps: 39.2\n",
      "epoch:109,  Loss: tensor([17.6242]), max_steps: 37.0\n",
      "epoch:110,  Loss: tensor([23.8504]), max_steps: 39.1\n",
      "epoch:111,  Loss: tensor([23.4114]), max_steps: 38.9\n",
      "epoch:112,  Loss: tensor([21.1393]), max_steps: 38.8\n",
      "epoch:113,  Loss: tensor([24.0098]), max_steps: 38.9\n",
      "epoch:114,  Loss: tensor([21.8828]), max_steps: 45.2\n",
      "epoch:115,  Loss: tensor([18.6252]), max_steps: 49.3\n",
      "epoch:116,  Loss: tensor([21.0405]), max_steps: 49.0\n",
      "epoch:117,  Loss: tensor([19.7827]), max_steps: 63.2\n",
      "epoch:118,  Loss: tensor([18.2073]), max_steps: 57.4\n",
      "epoch:119,  Loss: tensor([19.5411]), max_steps: 65.2\n",
      "epoch:120,  Loss: tensor([19.1677]), max_steps: 61.6\n",
      "epoch:121,  Loss: tensor([18.6202]), max_steps: 61.0\n",
      "epoch:122,  Loss: tensor([19.4950]), max_steps: 62.5\n",
      "epoch:123,  Loss: tensor([17.7024]), max_steps: 78.4\n",
      "epoch:124,  Loss: tensor([20.7289]), max_steps: 63.9\n",
      "epoch:125,  Loss: tensor([35.1336]), max_steps: 68.9\n",
      "epoch:126,  Loss: tensor([25.2168]), max_steps: 82.1\n",
      "epoch:127,  Loss: tensor([28.9755]), max_steps: 104.7\n",
      "epoch:128,  Loss: tensor([40.4341]), max_steps: 88.1\n",
      "epoch:129,  Loss: tensor([21.8388]), max_steps: 119.6\n",
      "epoch:130,  Loss: tensor([38.2692]), max_steps: 85.9\n",
      "epoch:131,  Loss: tensor([82.1171]), max_steps: 106.9\n",
      "epoch:132,  Loss: tensor([36.8431]), max_steps: 86.8\n",
      "epoch:133,  Loss: tensor([33.6704]), max_steps: 81.9\n",
      "epoch:134,  Loss: tensor([38.2982]), max_steps: 105.1\n",
      "epoch:135,  Loss: tensor([26.9376]), max_steps: 71.6\n",
      "epoch:136,  Loss: tensor([41.0735]), max_steps: 90.5\n",
      "epoch:137,  Loss: tensor([37.0287]), max_steps: 96.4\n",
      "epoch:138,  Loss: tensor([35.4810]), max_steps: 103.3\n",
      "epoch:139,  Loss: tensor([40.7308]), max_steps: 97.4\n",
      "epoch:140,  Loss: tensor([40.8731]), max_steps: 103.7\n",
      "epoch:141,  Loss: tensor([32.6144]), max_steps: 115.9\n",
      "epoch:142,  Loss: tensor([49.4970]), max_steps: 112.0\n",
      "epoch:143,  Loss: tensor([34.5903]), max_steps: 107.0\n",
      "epoch:144,  Loss: tensor([46.9547]), max_steps: 123.2\n",
      "epoch:145,  Loss: tensor([40.9038]), max_steps: 143.4\n",
      "epoch:146,  Loss: tensor([52.4963]), max_steps: 180.2\n",
      "epoch:147,  Loss: tensor([64.9303]), max_steps: 200.0\n",
      "epoch:148,  Loss: tensor([91.7571]), max_steps: 200.0\n",
      "epoch:149,  Loss: tensor([101.6609]), max_steps: 200.0\n",
      "epoch:150,  Loss: tensor([100.6282]), max_steps: 200.0\n",
      "epoch:151,  Loss: tensor([88.3217]), max_steps: 200.0\n",
      "epoch:152,  Loss: tensor([96.1193]), max_steps: 200.0\n",
      "epoch:153,  Loss: tensor([98.9113]), max_steps: 200.0\n",
      "epoch:154,  Loss: tensor([114.3084]), max_steps: 200.0\n",
      "epoch:155,  Loss: tensor([98.7264]), max_steps: 200.0\n",
      "epoch:156,  Loss: tensor([109.6983]), max_steps: 200.0\n",
      "epoch:157,  Loss: tensor([86.6804]), max_steps: 200.0\n",
      "epoch:158,  Loss: tensor([100.9003]), max_steps: 200.0\n",
      "epoch:159,  Loss: tensor([136.6542]), max_steps: 200.0\n",
      "epoch:160,  Loss: tensor([109.1622]), max_steps: 200.0\n",
      "epoch:161,  Loss: tensor([125.9881]), max_steps: 200.0\n",
      "epoch:162,  Loss: tensor([115.0393]), max_steps: 200.0\n",
      "epoch:163,  Loss: tensor([145.3211]), max_steps: 200.0\n",
      "epoch:164,  Loss: tensor([105.0594]), max_steps: 200.0\n",
      "epoch:165,  Loss: tensor([134.3993]), max_steps: 200.0\n",
      "epoch:166,  Loss: tensor([179.1526]), max_steps: 200.0\n",
      "epoch:167,  Loss: tensor([133.0999]), max_steps: 200.0\n",
      "epoch:168,  Loss: tensor([118.6051]), max_steps: 200.0\n",
      "epoch:169,  Loss: tensor([137.9389]), max_steps: 200.0\n",
      "epoch:170,  Loss: tensor([150.7376]), max_steps: 200.0\n",
      "epoch:171,  Loss: tensor([97.9862]), max_steps: 200.0\n",
      "epoch:172,  Loss: tensor([116.1463]), max_steps: 200.0\n",
      "epoch:173,  Loss: tensor([96.1581]), max_steps: 200.0\n",
      "epoch:174,  Loss: tensor([109.0369]), max_steps: 200.0\n",
      "epoch:175,  Loss: tensor([80.1925]), max_steps: 200.0\n",
      "epoch:176,  Loss: tensor([98.7280]), max_steps: 200.0\n",
      "epoch:177,  Loss: tensor([101.4102]), max_steps: 200.0\n",
      "epoch:178,  Loss: tensor([123.0334]), max_steps: 200.0\n",
      "epoch:179,  Loss: tensor([74.1381]), max_steps: 200.0\n",
      "epoch:180,  Loss: tensor([79.8466]), max_steps: 200.0\n",
      "epoch:181,  Loss: tensor([88.4202]), max_steps: 200.0\n",
      "epoch:182,  Loss: tensor([92.9927]), max_steps: 200.0\n",
      "epoch:183,  Loss: tensor([87.7424]), max_steps: 200.0\n",
      "epoch:184,  Loss: tensor([78.3882]), max_steps: 157.4\n",
      "epoch:185,  Loss: tensor([72.8031]), max_steps: 164.2\n",
      "epoch:186,  Loss: tensor([77.7178]), max_steps: 195.9\n",
      "epoch:187,  Loss: tensor([56.6041]), max_steps: 175.8\n",
      "epoch:188,  Loss: tensor([61.5382]), max_steps: 169.2\n",
      "epoch:189,  Loss: tensor([65.4137]), max_steps: 188.5\n",
      "epoch:190,  Loss: tensor([50.9551]), max_steps: 181.6\n",
      "epoch:191,  Loss: tensor([49.4484]), max_steps: 144.2\n",
      "epoch:192,  Loss: tensor([50.1975]), max_steps: 148.2\n",
      "epoch:193,  Loss: tensor([28.3327]), max_steps: 149.7\n",
      "epoch:194,  Loss: tensor([33.9319]), max_steps: 150.8\n",
      "epoch:195,  Loss: tensor([45.0349]), max_steps: 155.9\n",
      "epoch:196,  Loss: tensor([50.3044]), max_steps: 160.1\n",
      "epoch:197,  Loss: tensor([38.8114]), max_steps: 146.7\n",
      "epoch:198,  Loss: tensor([36.0938]), max_steps: 145.9\n",
      "epoch:199,  Loss: tensor([28.1837]), max_steps: 154.2\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 20\n",
    "gamma = 0.9\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch = generate_episode(env, batch_size, net)\n",
    "    l = loss(batch, gamma)\n",
    "\n",
    "    # 反向传播\n",
    "    opt.zero_grad()\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "\n",
    "    writer.add_scalars(\n",
    "        \"Loss\",\n",
    "        {\"loss\": l.item(), \"max_steps\": generate_episode(env, 10, net, predict=True)},\n",
    "        epoch,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"epoch:{},  Loss: {}, max_steps: {}\".format(\n",
    "            epoch, l.detach(), generate_episode(env, 10, net, predict=True)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e429e414-f88f-4625-8016-400ca0b06e8b",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f038155-9574-478d-8f9b-1b5d3c52a4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Python39\\lib\\site-packages\\gym\\wrappers\\record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at D:\\code\\python\\reinforcement_learnging\\gym_intro\\video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "D:\\Program Files\\Python39\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:59: UserWarning: \u001b[33mWARN: Disabling video recorder because environment <TimeLimit<OrderEnforcing<PassiveEnvChecker<CartPoleEnv<CartPole-v1>>>>> was not initialized with any compatible video mode between `rgb_array` and `rgb_array_list`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=\"video\")\n",
    "\n",
    "state, info = env.reset()\n",
    "total_rewards = 0\n",
    "\n",
    "while True:\n",
    "    p = net(torch.Tensor(state)).detach().numpy().reshape(-1)\n",
    "    action = np.random.choice(list(range(env.action_space.n)), p=p)\n",
    "    state, reward, terminated, truncted, info = env.step(action)\n",
    "    if terminated:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
