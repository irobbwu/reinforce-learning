{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77090bfe-2903-47d0-bca9-f2bf7fc29f13",
   "metadata": {},
   "source": [
    "# PTAN库\n",
    "\n",
    "宏观来讲，PTAN提供了下面的实体：Agent：知道如何将一批观察转换成一批需要执行的动作的类。\r\n",
    "\r\n",
    "它还可以包含可选状态，当需要在一个片段中为后续动作记录一些信息的时候可以用到。\r\n",
    "\r\n",
    "本库提供了好几个智能体用于最常见的一些RL场景，你也完全可以编写自己的BaseAgent子类。。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b5fed-1f0e-4ebd-a989-4803c57015f5",
   "metadata": {},
   "source": [
    "- ActionSelector：一小段与Agent协同工作的逻辑，它知道如何从网络的输出中选择动作。\n",
    "\n",
    "- ExperienceSource和它的变体：Agent的实例和Gym环境对象可以提供关于片段轨迹的信息。它最简单的形式就是每次一个（a, r,s'）状态转移，但其功能远不止如此。\n",
    "\n",
    "\n",
    "- ExperienceSourceBuffer和它的变体：具有各种特性的回放缓冲区。包含一个简单的回放缓冲区和两个版本的带优先级的回放缓冲区。各种工具类，比如TargetNet和用于时间序列预处理的包装器（用于在TensorBoard中追踪训练进度）。\n",
    "\n",
    "- PyTorch Ignite帮助类可以将PTAN集成到Ignite框架中去。Gym环境包装器，例如Atari游戏的包装器（从OpenAI Baselines复制而来，并做了一些调整）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abe9a3e-6cd5-467c-bb0a-09bcff7fa281",
   "metadata": {},
   "source": [
    "## 动作选择器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7593b23d-99a8-4be5-9264-adf9fc84bf31",
   "metadata": {},
   "source": [
    "- argmax：常被用在Q值方法中，也就是当用神经网络预测一组动作的Q值并需要一个Q(s, a)最大的动作时。\n",
    "\n",
    "- 基于策略的：网络的输出是概率分布（以logits的形式或归一化分布的形式），并且动作需要从这个分布采样。第4章已提到过这种情况，也就是讨论交叉熵方法的时候。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bfa7153d-efa4-4e41-a6d9-1f756c29e286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 8],\n",
       "       [2, 3, 7],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import ptan\n",
    "import torch.nn as nn\n",
    "\n",
    "l = list(range(9))\n",
    "random.shuffle(l)\n",
    "q_vals = np.array(l).reshape(3, 3)\n",
    "q_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f1c06-ed77-43e0-b063-73e824459ce9",
   "metadata": {},
   "source": [
    "\n",
    "ArgmaxActionSelector：对传入张量的第二维执行argmax。（它假设参数是一个矩阵，并且它的第一维为批维度。作的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac4ff307-a766-47db-9203-3218084cb8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "selector(q_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea08e93-237a-4b89-b97a-568f158dec4c",
   "metadata": {},
   "source": [
    "ProbabilityActionSeletor：从离散动作集的概率分布中采样。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0e368cb-ac3e-42c8-b8f6-85ba88dbdbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5671, 0.2177, 0.2152],\n",
       "        [0.2204, 0.5314, 0.2482],\n",
       "        [0.4703, 0.2981, 0.2316]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_vals = nn.Softmax(dim=1)(torch.Tensor(q_vals))\n",
    "q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7318943-79ae-4108-8f76-23169fbe8fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "selector(q_vals.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd40ef9-131e-4b8a-adc4-6b64a05301b1",
   "metadata": {},
   "source": [
    "EpsilonGreedyActionSelector：具有epsilon参数，用来指定选择随机动作的概率。\n",
    "\n",
    "epsilon大小代表随机的概率，若为0，则没有采取随即动作."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "960cc133-75d1-424e-9eb7-7a987a8c0d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.2)\n",
    "selector(q_vals.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3962fd-63ce-4ca7-bd5e-274204b7e476",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80fbbb3-642e-4b7e-9b89-fdaf36cea3ea",
   "metadata": {},
   "source": [
    "宏观来讲，智能体需要接受一批观察（以NumPy数组的形式）并返回一批它想执行的动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f930f2-8ac2-4cc1-b373-912fe8e44a0c",
   "metadata": {},
   "source": [
    "### DQNAgent\n",
    "当动作空间不是非常大的时候，这个类可以适用于Q-learning，包括Atari游戏和很多经典的问题。\r\n",
    "\r\n",
    "这个方法不是很通用，但本书的后面将介绍如何解决这个问题。\r\n",
    "\r\n",
    "DQNAgent需要一批观察（NumPy数组）作为输入，使用网络来获得Q值，然后使用提供的ActionSelector将Q值转换成动作的索引。\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9755e900-d455-4e77-bd01-c097aead1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNet(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, q_table_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, q_table_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "13e12124-c34c-4c3f-8aee-065ba2cbe129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1239,  0.0239],\n",
       "        [-0.0960,  0.1399],\n",
       "        [ 0.0616,  0.0046],\n",
       "        [ 0.0191,  0.1469],\n",
       "        [-0.1176,  0.2227]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DQNNet(4,16,2)\n",
    "net(torch.rand(5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f6f82a6f-22c6-4711-9181-973a75ca80f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1], dtype=int64), [None, None, None, None, None])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "agent = ptan.agent.DQNAgent(dqn_model=net, action_selector=selector)\n",
    "agent(torch.rand(5,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4a9e6-94c5-4421-84c3-0065c9ebb0b8",
   "metadata": {},
   "source": [
    "### PolicyAgent\n",
    "\n",
    "PolicyAgent需要神经网络生成离散动作集的策略分布。\r\n",
    "\r\n",
    "策略分布可以是logits（未归一化的）分布，也可以是归一化分布。\r\n",
    "\r\n",
    "实践中，最好都是用logits分布以提升训练过程的数值稳定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a418822e-253c-49a4-bc5b-6540eb45f704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6077, 1.8959, 1.6269, 1.7592],\n",
       "        [1.4544, 1.3852, 1.5736, 1.4986],\n",
       "        [1.0109, 1.5741, 1.1709, 1.0381],\n",
       "        [1.1970, 1.5519, 1.4666, 1.2554],\n",
       "        [1.9007, 1.8573, 1.8272, 1.6637]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.empty((5, 4)).uniform_(1,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "80496d10-4963-4ee3-8201-337df2d66080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 1, 1, 1]), [None, None, None, None, None])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "agent = ptan.agent.PolicyAgent(model=net, action_selector=selector,apply_softmax=True)\n",
    "agent(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae798041-1226-4bfa-afc3-4eb41507e2d5",
   "metadata": {},
   "source": [
    "## 经验源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1826af47-b7ae-40d3-b6dc-896805a57b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "747e4f88-7aa2-40cb-80f9-59f6d9b70c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment with observation 0..4 and actions 0..2\n",
    "    Observations are rotated sequentialy mod 5, reward is equal to given action.\n",
    "    Episodes are having fixed length of 10\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ToyEnv, self).__init__()\n",
    "        self.observation_space = gym.spaces.Discrete(n=5)\n",
    "        self.action_space = gym.spaces.Discrete(n=3)\n",
    "        self.step_index = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_index = 0\n",
    "        return self.step_index, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        is_done = self.step_index == 10\n",
    "        if is_done:\n",
    "            return self.step_index % self.observation_space.n, \\\n",
    "                   0.0, is_done, {}\n",
    "        self.step_index += 1\n",
    "        return self.step_index % self.observation_space.n, \\\n",
    "               float(action), self.step_index == 10, False, {}\n",
    "\n",
    "class DullAgent(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent always returns the fixed action\n",
    "    \"\"\"\n",
    "    def __init__(self, action: int):\n",
    "        self.action = action\n",
    "\n",
    "    def __call__(self, observations: List[Any],\n",
    "                 state: Optional[List] = None) \\\n",
    "            -> Tuple[List[int], Optional[List]]:\n",
    "        return [self.action for _ in observations], state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "89917891-7fc9-45da-9783-e8d5c0db34e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ToyEnv()\n",
    "agent = DullAgent(action=1)\n",
    "exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c6d7b9c6-a3c7-4a74-9bf9-74a333cc647b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[155], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, exp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(exp_source):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m15\u001b[39m:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Program Files\\Python39\\lib\\site-packages\\ptan\\experience.py:94\u001b[0m, in \u001b[0;36mExperienceSource.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m     next_state_n, r_n, is_done_n, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action_n)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m     next_state, r, is_done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action_n[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     95\u001b[0m     next_state_n, r_n, is_done_n \u001b[38;5;241m=\u001b[39m [next_state], [r], [is_done]\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ofs, (action, next_state, r, is_done) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(action_n, next_state_n, r_n, is_done_n)):\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "for idx, exp in enumerate(exp_source):\n",
    "    if idx > 15:\n",
    "        break\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a0c8a-9f19-4bfc-9b11-3c15b9bd4c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
