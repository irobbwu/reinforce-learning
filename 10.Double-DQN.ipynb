{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61bb43f-d5be-497c-b302-a7a76a6e9049",
   "metadata": {},
   "source": [
    "# 10.Double-DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665075ac-1cde-46de-b2b2-de898f685c4c",
   "metadata": {},
   "source": [
    "论文的作者建议使用训练网络来选择动作，但是使用目标网络的Q值。所以新的目标Q值为\n",
    "\n",
    "$$\n",
    "Q(s_t,a_t) = r_t+\\gamma Q'(s_{t+1}, argmaxQ(s_{t+1}, a))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6af8a5-f3db-452e-aca1-4415b6053543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gym.envs.toy_text import frozen_lake\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c325c7f-8676-4b7e-84f2-13c5dcccc175",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, q_table_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # 输入为状态，样本为（1*n）\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(hidden_size, q_table_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n,)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1ec6c-2405-4b4d-be91-61577e8fc211",
   "metadata": {},
   "source": [
    "# ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2face0-8a57-4e6a-af31-5378d5cb8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, queue_size, replay_time):\n",
    "        self.queue = []\n",
    "        self.queue_size = queue_size\n",
    "        self.replay_time = replay_time\n",
    "\n",
    "    def get_batch_queue(self, env, action_trigger, batch_size, epsilon):\n",
    "        def insert_sample_to_queue(env):\n",
    "            state, info = env.reset()\n",
    "            stop = 0\n",
    "\n",
    "            while True:\n",
    "                if np.random.uniform(0, 1, 1) > epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = action_trigger(state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                self.queue.append([state, action, next_state, reward, terminated])\n",
    "                state = next_state\n",
    "                if terminated:\n",
    "                    state, info = env.reset()\n",
    "                    stop += 1\n",
    "                    continue\n",
    "                if stop >= replay_time:\n",
    "                    break\n",
    "\n",
    "        def init_queue(env):\n",
    "            while True:\n",
    "                insert_sample_to_queue(env)\n",
    "                if len(self.queue) >= self.queue_size:\n",
    "                    break\n",
    "\n",
    "        init_queue(env)\n",
    "        insert_sample_to_queue(env)\n",
    "        self.queue = self.queue[-self.queue_size :]\n",
    "\n",
    "        return random.sample(self.queue, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be83a26-89f6-4c9a-9185-1dc7203612c6",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f672a2bf-05df-418c-91a4-d661f35087b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, obs_size, hidden_size, q_table_size):\n",
    "        self.env = env\n",
    "        self.net = Net(obs_size, hidden_size, q_table_size)\n",
    "        self.tgt_net = Net(obs_size, hidden_size, q_table_size)\n",
    "\n",
    "    # 更新net参数\n",
    "    def update_net_parameters(self, update=True):\n",
    "        self.net.load_state_dict(self.tgt_net.state_dict())\n",
    "\n",
    "    def get_action_trigger(self, state):\n",
    "        state = torch.Tensor(state)\n",
    "        action = int(torch.argmax(self.tgt_net(state).detach()))\n",
    "        return action\n",
    "\n",
    "    # 计算y_hat_and_y\n",
    "    def calculate_y_hat_and_y(self, batch, gamma):\n",
    "        y = []\n",
    "        action_sapce = []\n",
    "        state_sapce = []\n",
    "        ## \n",
    "        for state, action, next_state, reward, terminated in batch:\n",
    "            q_table_net = self.net(torch.Tensor(next_state)).detach()\n",
    "            ## double DQN\n",
    "            tgt_net_action = self.get_action_trigger(next_state)\n",
    "            y.append(reward + (1 - terminated) * gamma * float(q_table_net[tgt_net_action]))\n",
    "            action_sapce.append(action)\n",
    "            state_sapce.append(state)\n",
    "        y_hat = self.tgt_net(torch.Tensor(np.array(state_sapce)))\n",
    "        y_hat = y_hat.gather(1, torch.LongTensor(action_sapce).reshape(-1, 1))\n",
    "        return y_hat.reshape(-1), torch.tensor(y)\n",
    "\n",
    "    def predict_reward(self):\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        reward_space = []\n",
    "\n",
    "        while True:\n",
    "            step += 1\n",
    "            state = torch.Tensor(state)\n",
    "            action = int(torch.argmax(self.net(state).detach()))\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            reward_space.append(reward)\n",
    "            state = next_state\n",
    "            if terminated:\n",
    "                state, info = env.reset()\n",
    "                continue\n",
    "            if step >= 100:\n",
    "                break\n",
    "        return float(np.mean(reward_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c65fde-9f1d-4f36-b79b-e06a314ad5bf",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d34d36-fe18-4790-9194-46818a4b9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "queue_size = 500\n",
    "replay_time = 50\n",
    "\n",
    "## 初始化环境\n",
    "env = frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "env.spec = gym.spec(\"FrozenLake-v1\")\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "env = DiscreteOneHotWrapper(env)\n",
    "\n",
    "## 初始化buffer\n",
    "replay_buffer = ReplayBuffer(queue_size, replay_time)\n",
    "\n",
    "## 初始化dqn\n",
    "obs_size = env.observation_space.shape[0]\n",
    "q_table_size = env.action_space.n\n",
    "dqn = DQN(env, obs_size, hidden_size, q_table_size)\n",
    "\n",
    "# 定义优化器\n",
    "opt = optim.Adam(dqn.tgt_net.parameters(), lr=0.01)\n",
    "\n",
    "# 定义损失函数\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"logs/DQN/Double_DQN_FrozenLake\", comment=\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd682d3-7daf-460f-a040-4093ddc6b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epsilon = 0.8\n",
    "epochs = 500\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f59cea-35d7-49f9-8296-4c203c10fef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,  MSE: 0.002034767996519804, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:1,  MSE: 0.01553338672965765, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:2,  MSE: 0.0041157943196594715, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:3,  MSE: 0.005832789000123739, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:4,  MSE: 0.004888113122433424, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:5,  MSE: 0.009765115566551685, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:6,  MSE: 0.0090354448184371, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:7,  MSE: 0.007912073284387589, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:8,  MSE: 0.005203092470765114, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:9,  MSE: 0.004023554269224405, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:10,  MSE: 0.004651330411434174, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:11,  MSE: 0.004799548536539078, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:12,  MSE: 0.004201455507427454, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:13,  MSE: 0.003958794288337231, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:14,  MSE: 0.0025175316259264946, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:15,  MSE: 0.0028849358204752207, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:16,  MSE: 0.0014214962720870972, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:17,  MSE: 0.0006082609761506319, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:18,  MSE: 0.003648413810878992, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:19,  MSE: 0.002087283181026578, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:20,  MSE: 0.030480068176984787, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:21,  MSE: 0.002394896000623703, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:22,  MSE: 0.0017063874984160066, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:23,  MSE: 0.002327415393665433, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:24,  MSE: 0.0023473717737942934, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:25,  MSE: 0.004622491542249918, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:26,  MSE: 0.00524283479899168, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:27,  MSE: 0.002107753185555339, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:28,  MSE: 0.0030779915396124125, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:29,  MSE: 0.003976242616772652, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:30,  MSE: 0.0021762410178780556, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:31,  MSE: 0.0024410109035670757, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:32,  MSE: 0.0013566863490268588, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:33,  MSE: 0.0009289016597904265, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:34,  MSE: 0.0005472715711221099, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:35,  MSE: 0.00044298404827713966, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:36,  MSE: 0.0004801741451956332, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:37,  MSE: 0.0002608236391097307, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:38,  MSE: 0.005064487457275391, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:39,  MSE: 0.003436849219724536, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:40,  MSE: 0.0015725151170045137, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:41,  MSE: 0.0005094775697216392, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:42,  MSE: 0.000893252668902278, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:43,  MSE: 0.0009495537960901856, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:44,  MSE: 0.001434425124898553, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:45,  MSE: 0.0019608645234256983, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:46,  MSE: 0.0012133632553741336, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:47,  MSE: 0.0013929365668445826, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:48,  MSE: 0.011269571259617805, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:49,  MSE: 0.001252321875654161, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:50,  MSE: 0.003834926290437579, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:51,  MSE: 0.0009386247838847339, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:52,  MSE: 0.003607102669775486, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:53,  MSE: 0.014618332497775555, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:54,  MSE: 0.010466992855072021, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:55,  MSE: 0.012871859595179558, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:56,  MSE: 0.017576292157173157, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:57,  MSE: 0.010562386363744736, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:58,  MSE: 0.006935103330761194, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:59,  MSE: 0.0082581527531147, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:60,  MSE: 0.008252326399087906, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:61,  MSE: 0.010721283033490181, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:62,  MSE: 0.009713702835142612, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:63,  MSE: 0.0022494664881378412, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:64,  MSE: 0.003420073539018631, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:65,  MSE: 0.0022310444619506598, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:66,  MSE: 0.0007832598639652133, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:67,  MSE: 0.000491136044729501, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:68,  MSE: 0.0008141922880895436, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:69,  MSE: 0.0010014233412221074, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:70,  MSE: 0.0007650272455066442, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:71,  MSE: 0.0007801188621670008, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:72,  MSE: 0.001036816625855863, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:73,  MSE: 0.008322097361087799, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:74,  MSE: 0.004499277099967003, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:75,  MSE: 0.00832504965364933, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:76,  MSE: 0.10094964504241943, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:77,  MSE: 0.08553922921419144, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:78,  MSE: 0.023191988468170166, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:79,  MSE: 0.01984190009534359, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:80,  MSE: 0.018312396481633186, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:81,  MSE: 0.005107814446091652, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:82,  MSE: 0.008167251944541931, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:83,  MSE: 0.004834411200135946, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:84,  MSE: 0.0021827879827469587, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:85,  MSE: 0.0013203425332903862, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:86,  MSE: 0.006918972823768854, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:87,  MSE: 0.007134481333196163, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:88,  MSE: 0.004365264438092709, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:89,  MSE: 0.003650267142802477, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:90,  MSE: 0.015113811939954758, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:91,  MSE: 0.06476995348930359, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:92,  MSE: 0.04887305200099945, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:93,  MSE: 0.03780052065849304, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:94,  MSE: 0.0272965244948864, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:95,  MSE: 0.012832310982048512, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:96,  MSE: 0.015218034386634827, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:97,  MSE: 0.0066500697284936905, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:98,  MSE: 0.00818593055009842, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:99,  MSE: 0.005438734777271748, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:100,  MSE: 0.0035480717197060585, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:101,  MSE: 0.008805032819509506, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:102,  MSE: 0.008929802104830742, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:103,  MSE: 0.03693468123674393, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:104,  MSE: 0.02548125572502613, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:105,  MSE: 0.025281816720962524, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:106,  MSE: 0.025753051042556763, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:107,  MSE: 0.003997209947556257, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:108,  MSE: 0.0044441185891628265, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:109,  MSE: 0.004939565900713205, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:110,  MSE: 0.004328524693846703, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:111,  MSE: 0.007614633999764919, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:112,  MSE: 0.005948372185230255, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:113,  MSE: 0.005016747396439314, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:114,  MSE: 0.02813677489757538, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:115,  MSE: 0.016042323783040047, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:116,  MSE: 0.007365528028458357, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:117,  MSE: 0.02808276377618313, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:118,  MSE: 0.03103337436914444, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:119,  MSE: 0.034470848739147186, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:120,  MSE: 0.032760169357061386, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:121,  MSE: 0.0811997577548027, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:122,  MSE: 0.070779949426651, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:123,  MSE: 0.058823950588703156, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:124,  MSE: 0.08902856707572937, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:125,  MSE: 0.030540762469172478, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:126,  MSE: 0.026927653700113297, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:127,  MSE: 0.0183861143887043, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:128,  MSE: 0.06075850874185562, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:129,  MSE: 0.18819807469844818, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:130,  MSE: 0.12166431546211243, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:131,  MSE: 0.056440722197294235, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:132,  MSE: 0.035212356597185135, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:133,  MSE: 0.024114590138196945, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:134,  MSE: 0.01834944449365139, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:135,  MSE: 0.022314388304948807, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:136,  MSE: 0.016758045181632042, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:137,  MSE: 0.004502842668443918, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:138,  MSE: 0.0068974667228758335, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:139,  MSE: 0.004248945042490959, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:140,  MSE: 0.009679671376943588, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:141,  MSE: 0.005549395456910133, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:142,  MSE: 0.012356751598417759, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:143,  MSE: 0.013457775115966797, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:144,  MSE: 0.005234209820628166, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:145,  MSE: 0.002102052327245474, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:146,  MSE: 0.0030986389610916376, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:147,  MSE: 0.001872122986242175, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:148,  MSE: 0.012941654771566391, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:149,  MSE: 0.005695969797670841, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:150,  MSE: 0.009749888442456722, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:151,  MSE: 0.007888130843639374, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:152,  MSE: 0.003604449098929763, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:153,  MSE: 0.0055224718526005745, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:154,  MSE: 0.005874975584447384, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:155,  MSE: 0.0050003910437226295, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:156,  MSE: 0.010058825835585594, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:157,  MSE: 0.011489557102322578, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:158,  MSE: 0.005336904898285866, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:159,  MSE: 0.02019602805376053, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:160,  MSE: 0.012597354128956795, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:161,  MSE: 0.001202662824653089, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:162,  MSE: 0.005914194975048304, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:163,  MSE: 0.011549390852451324, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:164,  MSE: 0.02289711683988571, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:165,  MSE: 0.003824392566457391, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:166,  MSE: 0.017591476440429688, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:167,  MSE: 0.004597872961312532, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:168,  MSE: 0.023691704496741295, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:169,  MSE: 0.006327033508569002, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:170,  MSE: 0.00629845354706049, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:171,  MSE: 0.005270336754620075, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:172,  MSE: 0.01472276821732521, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:173,  MSE: 0.006591699086129665, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:174,  MSE: 0.011075044982135296, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:175,  MSE: 0.010951523669064045, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:176,  MSE: 0.010437875054776669, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:177,  MSE: 0.006946535315364599, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:178,  MSE: 0.01776542142033577, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:179,  MSE: 0.009423606097698212, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:180,  MSE: 0.026288682594895363, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:181,  MSE: 0.012569339945912361, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:182,  MSE: 0.00923660397529602, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:183,  MSE: 0.0024678318295627832, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:184,  MSE: 0.0008636594284325838, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:185,  MSE: 0.0009912600507959723, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:186,  MSE: 0.000540644396096468, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:187,  MSE: 0.000615238503087312, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:188,  MSE: 0.0005455336649902165, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:189,  MSE: 0.0005940876435488462, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:190,  MSE: 0.0017842290690168738, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:191,  MSE: 0.0013325305189937353, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:192,  MSE: 0.0010883546201512218, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:193,  MSE: 0.0008525534067302942, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:194,  MSE: 0.0009655823814682662, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:195,  MSE: 0.0009913273388519883, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:196,  MSE: 0.0007820193422958255, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:197,  MSE: 0.0005659121088683605, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:198,  MSE: 0.0007758092251606286, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:199,  MSE: 0.0006368100875988603, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:200,  MSE: 0.0007221191190183163, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:201,  MSE: 0.0005632652319036424, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:202,  MSE: 0.00040767621248960495, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:203,  MSE: 0.0006661304505541921, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:204,  MSE: 0.0003994226281065494, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:205,  MSE: 0.00046303108683787286, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:206,  MSE: 0.0004374869749881327, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:207,  MSE: 0.0002182213356718421, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:208,  MSE: 0.00026905129197984934, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:209,  MSE: 0.0002500840637367219, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:210,  MSE: 0.00016677546955179423, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:211,  MSE: 0.00015835667727515101, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:212,  MSE: 0.00022542155056726187, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:213,  MSE: 0.0001595404464751482, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:214,  MSE: 0.00012814167712349445, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:215,  MSE: 6.69481378281489e-05, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:216,  MSE: 0.000714336521923542, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:217,  MSE: 0.000101163299405016, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:218,  MSE: 0.00014816151815466583, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:219,  MSE: 0.0001569943269714713, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:220,  MSE: 0.00021650247799698263, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:221,  MSE: 0.00020640980801545084, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:222,  MSE: 0.00018772980547510087, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:223,  MSE: 0.000137228867970407, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:224,  MSE: 0.00010769552318379283, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:225,  MSE: 0.0003350701299495995, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:226,  MSE: 0.0005088710458949208, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:227,  MSE: 0.0002871542237699032, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:228,  MSE: 0.00022038505994714797, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:229,  MSE: 0.0002984312013722956, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:230,  MSE: 0.0005265526124276221, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:231,  MSE: 0.0006698062643408775, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:232,  MSE: 0.0006149267428554595, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:233,  MSE: 0.0003059029404539615, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:234,  MSE: 0.00018384947907179594, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:235,  MSE: 0.00019014031568076462, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:236,  MSE: 0.0004923719679936767, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:237,  MSE: 0.002267607022076845, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:238,  MSE: 0.0033508010674268007, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:239,  MSE: 0.00449514901265502, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:240,  MSE: 0.00023176753893494606, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:241,  MSE: 0.0006677629426121712, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:242,  MSE: 0.0005377214984036982, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:243,  MSE: 0.0006361682899296284, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:244,  MSE: 0.0012742667458951473, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:245,  MSE: 0.0012951847165822983, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:246,  MSE: 0.001936479122377932, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:247,  MSE: 0.0016412856057286263, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:248,  MSE: 0.0003851461224257946, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:249,  MSE: 0.00024498841958120465, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:250,  MSE: 0.00013894915173295885, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:251,  MSE: 0.0002186858473578468, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:252,  MSE: 0.0006150445551611483, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:253,  MSE: 0.0002094795781886205, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:254,  MSE: 0.00042392389150336385, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:255,  MSE: 0.00043088194797746837, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:256,  MSE: 0.00013914689770899713, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:257,  MSE: 0.00011853055184474215, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:258,  MSE: 7.665322482353076e-05, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:259,  MSE: 0.0004977297503501177, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:260,  MSE: 0.0002227990044048056, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:261,  MSE: 0.00011692219413816929, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:262,  MSE: 0.00014636400737799704, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:263,  MSE: 0.00011023883416783065, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:264,  MSE: 9.319563105236739e-05, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:265,  MSE: 0.00013718426635023206, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:266,  MSE: 7.823939085938036e-05, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:267,  MSE: 0.00010539405047893524, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:268,  MSE: 0.00029690482188016176, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:269,  MSE: 0.0006026435876265168, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:270,  MSE: 0.00013372563989832997, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:271,  MSE: 0.0002597777347546071, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:272,  MSE: 0.0002239555469714105, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:273,  MSE: 0.00016490121197421104, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:274,  MSE: 0.000497809611260891, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:275,  MSE: 0.006935345474630594, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:276,  MSE: 0.015499008819460869, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:277,  MSE: 0.010402537882328033, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:278,  MSE: 0.02433387190103531, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:279,  MSE: 0.04102744534611702, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:280,  MSE: 0.03636086359620094, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:281,  MSE: 0.00343026639893651, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:282,  MSE: 0.0007213385542854667, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:283,  MSE: 0.0018298989161849022, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:284,  MSE: 0.000953833747189492, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:285,  MSE: 0.0019356845878064632, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:286,  MSE: 0.001022904529236257, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:287,  MSE: 0.0007661631098017097, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:288,  MSE: 0.0013471008278429508, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:289,  MSE: 0.0030411970801651478, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:290,  MSE: 0.002479531103745103, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:291,  MSE: 0.0038649491034448147, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:292,  MSE: 0.0024597137235105038, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:293,  MSE: 0.0021616641897708178, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:294,  MSE: 0.0037879599258303642, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:295,  MSE: 0.0036592441610991955, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:296,  MSE: 0.003409218043088913, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:297,  MSE: 0.006543380208313465, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:298,  MSE: 0.003924489486962557, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:299,  MSE: 0.0033108824864029884, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:300,  MSE: 0.010446530766785145, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:301,  MSE: 0.0207755696028471, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:302,  MSE: 0.01755530573427677, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:303,  MSE: 0.011154595762491226, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:304,  MSE: 0.010469407774508, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:305,  MSE: 0.009611926041543484, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:306,  MSE: 0.01218191348016262, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:307,  MSE: 0.009982259944081306, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:308,  MSE: 0.006315411068499088, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:309,  MSE: 0.005011360626667738, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:310,  MSE: 0.00444001704454422, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:311,  MSE: 0.020227717235684395, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:312,  MSE: 0.019122513011097908, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:313,  MSE: 0.015975376591086388, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:314,  MSE: 0.008804239332675934, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:315,  MSE: 0.006561085116118193, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:316,  MSE: 0.0036139865405857563, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:317,  MSE: 0.005494485609233379, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:318,  MSE: 0.0061667608097195625, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:319,  MSE: 0.005905709229409695, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:320,  MSE: 0.006151089444756508, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:321,  MSE: 0.018061863258481026, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:322,  MSE: 0.013000264763832092, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:323,  MSE: 0.012612471356987953, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:324,  MSE: 0.01111589465290308, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:325,  MSE: 0.010073494166135788, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:326,  MSE: 0.008594481274485588, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:327,  MSE: 0.005292275454849005, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:328,  MSE: 0.004773102700710297, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:329,  MSE: 0.00433005765080452, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:330,  MSE: 0.004338111728429794, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:331,  MSE: 0.0032232673838734627, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:332,  MSE: 0.002311290707439184, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:333,  MSE: 0.003322076518088579, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:334,  MSE: 0.0014209095388650894, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:335,  MSE: 0.0025734652299433947, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:336,  MSE: 0.0013316510012373328, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:337,  MSE: 0.0006946530193090439, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:338,  MSE: 0.0022424038033932447, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:339,  MSE: 0.0008749679545871913, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:340,  MSE: 0.004147668369114399, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:341,  MSE: 0.0031328764744102955, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:342,  MSE: 0.0017830801662057638, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:343,  MSE: 0.0050355507992208, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:344,  MSE: 0.0012882424052804708, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:345,  MSE: 0.001821626559831202, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:346,  MSE: 0.015849022194743156, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:347,  MSE: 0.013398300856351852, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:348,  MSE: 0.06350777298212051, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:349,  MSE: 0.05035386607050896, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:350,  MSE: 0.03908168151974678, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:351,  MSE: 0.014862582087516785, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:352,  MSE: 0.02085140161216259, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:353,  MSE: 0.01200027298182249, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:354,  MSE: 0.019635530188679695, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:355,  MSE: 0.019544869661331177, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:356,  MSE: 0.015541482716798782, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:357,  MSE: 0.011198066174983978, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:358,  MSE: 0.008217129856348038, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:359,  MSE: 0.007797902449965477, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:360,  MSE: 0.004825549200177193, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:361,  MSE: 0.02161085046827793, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:362,  MSE: 0.02196795679628849, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:363,  MSE: 0.0239899642765522, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:364,  MSE: 0.016344904899597168, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:365,  MSE: 0.03790158033370972, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:366,  MSE: 0.012034549377858639, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:367,  MSE: 0.008808073587715626, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:368,  MSE: 0.007454837206751108, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:369,  MSE: 0.005808823276311159, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:370,  MSE: 0.004425691906362772, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:371,  MSE: 0.01966073550283909, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:372,  MSE: 0.014849853701889515, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:373,  MSE: 0.018919769674539566, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:374,  MSE: 0.023786939680576324, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:375,  MSE: 0.017209824174642563, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:376,  MSE: 0.02021482214331627, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:377,  MSE: 0.011677038855850697, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:378,  MSE: 0.013516402803361416, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:379,  MSE: 0.010069422423839569, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:380,  MSE: 0.011883161962032318, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:381,  MSE: 0.021707642823457718, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:382,  MSE: 0.022718053311109543, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:383,  MSE: 0.01942322961986065, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:384,  MSE: 0.02218366041779518, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:385,  MSE: 0.018418779596686363, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:386,  MSE: 0.015328667126595974, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:387,  MSE: 0.014320829883217812, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:388,  MSE: 0.014893154613673687, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:389,  MSE: 0.029068754985928535, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:390,  MSE: 0.009356404654681683, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:391,  MSE: 0.009309066459536552, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:392,  MSE: 0.006136506795883179, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:393,  MSE: 0.0048594046384096146, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:394,  MSE: 0.005361379589885473, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:395,  MSE: 0.008120718412101269, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:396,  MSE: 0.004936840385198593, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:397,  MSE: 0.004002322442829609, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:398,  MSE: 0.005493014119565487, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:399,  MSE: 0.0036155942361801863, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:400,  MSE: 0.002823269460350275, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:401,  MSE: 0.007171758450567722, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:402,  MSE: 0.006561958696693182, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:403,  MSE: 0.0044502937234938145, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:404,  MSE: 0.0033600933384150267, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:405,  MSE: 0.0022524138912558556, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:406,  MSE: 0.002618987113237381, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:407,  MSE: 0.003402002388611436, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:408,  MSE: 0.003266772720962763, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:409,  MSE: 0.0032188103068619967, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:410,  MSE: 0.0027397898957133293, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:411,  MSE: 0.010946922935545444, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:412,  MSE: 0.008417252451181412, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:413,  MSE: 0.005603030323982239, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:414,  MSE: 0.003645841032266617, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:415,  MSE: 0.0031296578235924244, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:416,  MSE: 0.002782850991934538, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:417,  MSE: 0.0029145621228963137, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:418,  MSE: 0.004545210395008326, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:419,  MSE: 0.005267549306154251, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:420,  MSE: 0.004037612583488226, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:421,  MSE: 0.02836165577173233, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:422,  MSE: 0.023027390241622925, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:423,  MSE: 0.018868891522288322, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:424,  MSE: 0.0134771429002285, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:425,  MSE: 0.02110671065747738, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:426,  MSE: 0.008911460638046265, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:427,  MSE: 0.006669288035482168, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:428,  MSE: 0.003329249331727624, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:429,  MSE: 0.0029015180189162493, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:430,  MSE: 0.0030150252860039473, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:431,  MSE: 0.012332148849964142, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:432,  MSE: 0.004160391632467508, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:433,  MSE: 0.004736576694995165, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:434,  MSE: 0.005803513340651989, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:435,  MSE: 0.003130397293716669, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:436,  MSE: 0.006567040458321571, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:437,  MSE: 0.0048579503782093525, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:438,  MSE: 0.004964502528309822, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:439,  MSE: 0.0040333205834031105, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:440,  MSE: 0.005466417409479618, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:441,  MSE: 0.00450316583737731, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:442,  MSE: 0.0034819103311747313, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:443,  MSE: 0.002509482903406024, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:444,  MSE: 0.0051783062517642975, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:445,  MSE: 0.0035399028565734625, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:446,  MSE: 0.00396708445623517, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:447,  MSE: 0.003012536559253931, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:448,  MSE: 0.0034287164453417063, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:449,  MSE: 0.001990349730476737, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:450,  MSE: 0.005366108845919371, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:451,  MSE: 0.0034089002292603254, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:452,  MSE: 0.004179125186055899, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:453,  MSE: 0.0021034872625023127, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:454,  MSE: 0.006532507948577404, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:455,  MSE: 0.006249581929296255, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:456,  MSE: 0.012976597063243389, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:457,  MSE: 0.010297255590558052, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:458,  MSE: 0.010820267722010612, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:459,  MSE: 0.006444389931857586, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:460,  MSE: 0.003901528427377343, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:461,  MSE: 0.011018729768693447, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:462,  MSE: 0.011377106420695782, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:463,  MSE: 0.007004700601100922, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:464,  MSE: 0.009444366209208965, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:465,  MSE: 0.005906823556870222, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:466,  MSE: 0.005562004167586565, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:467,  MSE: 0.007277727127075195, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:468,  MSE: 0.009516971185803413, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:469,  MSE: 0.007779516279697418, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:470,  MSE: 0.006539170630276203, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:471,  MSE: 0.017751744017004967, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:472,  MSE: 0.014305890537798405, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:473,  MSE: 0.014920514076948166, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:474,  MSE: 0.009733146987855434, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:475,  MSE: 0.007994608953595161, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:476,  MSE: 0.006846853066235781, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:477,  MSE: 0.004829366225749254, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:478,  MSE: 0.004589175805449486, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:479,  MSE: 0.006443797145038843, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:480,  MSE: 0.005467339418828487, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:481,  MSE: 0.04477253183722496, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:482,  MSE: 0.03981548920273781, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:483,  MSE: 0.03545728325843811, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:484,  MSE: 0.030885277315974236, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:485,  MSE: 0.022776640951633453, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:486,  MSE: 0.019610300660133362, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:487,  MSE: 0.013706237077713013, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:488,  MSE: 0.009353149682283401, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:489,  MSE: 0.00755205238237977, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:490,  MSE: 0.007708100602030754, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:491,  MSE: 0.027105513960123062, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:492,  MSE: 0.026767944917082787, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:493,  MSE: 0.029118765145540237, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:494,  MSE: 0.025382526218891144, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:495,  MSE: 0.023036446422338486, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:496,  MSE: 0.027638444676995277, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:497,  MSE: 0.026208920404314995, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:498,  MSE: 0.021712953224778175, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:499,  MSE: 0.01811465248465538, epsilon: 0.8, 100 steps reward: 0.16\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    batch = replay_buffer.get_batch_queue(\n",
    "        env, dqn.get_action_trigger, batch_size, epsilon\n",
    "    )\n",
    "    y_hat, y = dqn.calculate_y_hat_and_y(batch, gamma)\n",
    "    l = loss(y_hat, y)\n",
    "\n",
    "    # 反向传播\n",
    "    opt.zero_grad()\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        dqn.update_net_parameters()\n",
    "\n",
    "    predict_reward = dqn.predict_reward()\n",
    "    writer.add_scalars(\n",
    "        \"MSE\", {\"loss\": l.item(), \"predict_reward\": predict_reward}, epoch\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"epoch:{},  MSE: {}, epsilon: {}, 100 steps reward: {}\".format(\n",
    "            epoch, l, epsilon, predict_reward\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cccd2e9-a413-4982-b071-68aa3690d897",
   "metadata": {},
   "source": [
    "# 可视化预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82486b91-4d0f-44bd-acbc-46bc9668efcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "DQN_Q = dqn.net\n",
    "\n",
    "env = frozen_lake.FrozenLakeEnv(is_slippery=False, render_mode=\"human\")\n",
    "env.spec = gym.spec(\"FrozenLake-v1\")\n",
    "# display_size = 512\n",
    "# env.window_size = (display_size, display_size)\n",
    "# env.cell_size = (\n",
    "#     env.window_size[0] // env.ncol,\n",
    "#     env.window_size[1] // env.nrow,\n",
    "# )\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=\"video\")\n",
    "\n",
    "env = DiscreteOneHotWrapper(env)\n",
    "\n",
    "state, info = env.reset()\n",
    "total_rewards = 0\n",
    "\n",
    "while True:\n",
    "    action = int(torch.argmax(DQN_Q(torch.Tensor(state))))\n",
    "    state, reward, terminated, truncted, info = env.step(action)\n",
    "    print(action)\n",
    "    if terminated:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db355b72-f95d-4d64-8008-c90dc116b9de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
