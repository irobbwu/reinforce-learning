{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3536a49-821e-48ff-b201-ff0dbcb8b114",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from collections import namedtuple\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gym.envs.toy_text import frozen_lake\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n,)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "598fb6c3-fc4b-4d7c-a4fa-3c999452de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple(\"Episode\", field_names=[\"reward\", \"steps\"])\n",
    "EpisodeStep = namedtuple(\"EpisodeStep\", field_names=[\"observation\", \"action\"])\n",
    "EpisodeStepReward = namedtuple(\"EpisodeStepReward\", field_names=[\"reward\"])\n",
    "\n",
    "\n",
    "# 批处理\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = []\n",
    "    episode_steps = []\n",
    "    # 该状态的状态值\n",
    "    obs, info = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    # while True:\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor(obs.reshape(1, -1))\n",
    "        act_probs_y = sm(net(obs_v))\n",
    "        act_probs = act_probs_y.detach().numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        ## 更新 step\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "\n",
    "        ## 更新 reward\n",
    "        episode_reward.append(reward)\n",
    "\n",
    "        if terminated:\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = []\n",
    "            episode_steps = []\n",
    "            next_obs, info = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                env\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def discount_reward(r_history, gamma):\n",
    "    n = len(r_history)\n",
    "    dr = 0\n",
    "    for i in range(n):\n",
    "        dr += gamma**i * r_history[i]\n",
    "    return dr\n",
    "\n",
    "\n",
    "# 筛选批\n",
    "def filter_batch(batch, percentile):\n",
    "    filter_fun = lambda s: discount_reward(s.reward, GAMMA)\n",
    "    disc_rewards = list(map(filter_fun, batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "    return elite_batch, train_obs, train_act, reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c69a73-36dd-4517-8fb5-5498075483e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(12345)\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 200\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9\n",
    "\n",
    "env = frozen_lake.FrozenLakeEnv(is_slippery=False, render_mode=\"human\")\n",
    "env.spec = gym.spec(\"FrozenLake-v1\")\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "env = DiscreteOneHotWrapper(env)\n",
    "# env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "# env = gym.wrappers.RecordVideo(\n",
    "#     env, video_folder=\"video\", name_prefix=\"mario\", video_length=20\n",
    "# )\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "objective = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d43b2-239b-4654-90d9-2e9a5b37dbc4",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss=1.385, rw_mean=0.002, rw_bound=0.000, batch=5\n",
      "batch 1: loss=1.378, rw_mean=0.002, rw_bound=0.000, batch=9\n",
      "batch 2: loss=1.375, rw_mean=0.001, rw_bound=0.000, batch=11\n",
      "batch 3: loss=1.371, rw_mean=0.000, rw_bound=0.000, batch=11\n",
      "batch 4: loss=1.367, rw_mean=0.002, rw_bound=0.000, batch=15\n",
      "batch 5: loss=1.357, rw_mean=0.003, rw_bound=0.000, batch=21\n",
      "batch 6: loss=1.359, rw_mean=0.001, rw_bound=0.000, batch=24\n",
      "batch 7: loss=1.354, rw_mean=0.001, rw_bound=0.000, batch=27\n",
      "batch 8: loss=1.343, rw_mean=0.004, rw_bound=0.000, batch=36\n",
      "batch 9: loss=1.337, rw_mean=0.002, rw_bound=0.000, batch=40\n",
      "batch 10: loss=1.332, rw_mean=0.000, rw_bound=0.000, batch=41\n",
      "batch 11: loss=1.330, rw_mean=0.004, rw_bound=0.000, batch=49\n",
      "batch 12: loss=1.325, rw_mean=0.001, rw_bound=0.000, batch=51\n",
      "batch 13: loss=1.318, rw_mean=0.003, rw_bound=0.000, batch=56\n",
      "batch 14: loss=1.313, rw_mean=0.002, rw_bound=0.000, batch=59\n",
      "batch 15: loss=1.306, rw_mean=0.006, rw_bound=0.000, batch=71\n",
      "batch 16: loss=1.304, rw_mean=0.002, rw_bound=0.000, batch=78\n",
      "batch 17: loss=1.298, rw_mean=0.004, rw_bound=0.000, batch=87\n",
      "batch 18: loss=1.293, rw_mean=0.003, rw_bound=0.000, batch=92\n",
      "batch 19: loss=1.287, rw_mean=0.007, rw_bound=0.000, batch=106\n",
      "batch 20: loss=1.280, rw_mean=0.006, rw_bound=0.000, batch=118\n",
      "batch 21: loss=1.271, rw_mean=0.007, rw_bound=0.000, batch=129\n",
      "batch 22: loss=1.264, rw_mean=0.008, rw_bound=0.000, batch=144\n",
      "batch 23: loss=1.260, rw_mean=0.005, rw_bound=0.000, batch=155\n",
      "batch 24: loss=1.256, rw_mean=0.002, rw_bound=0.000, batch=160\n",
      "batch 25: loss=1.253, rw_mean=0.005, rw_bound=0.000, batch=170\n",
      "batch 26: loss=1.246, rw_mean=0.008, rw_bound=0.000, batch=184\n",
      "batch 27: loss=1.244, rw_mean=0.006, rw_bound=0.000, batch=197\n",
      "batch 28: loss=1.238, rw_mean=0.005, rw_bound=0.000, batch=207\n",
      "batch 29: loss=1.232, rw_mean=0.006, rw_bound=0.000, batch=217\n",
      "batch 30: loss=1.227, rw_mean=0.007, rw_bound=0.000, batch=231\n",
      "batch 31: loss=1.225, rw_mean=0.006, rw_bound=0.000, batch=245\n",
      "batch 32: loss=1.223, rw_mean=0.009, rw_bound=0.000, batch=263\n",
      "batch 33: loss=1.213, rw_mean=0.014, rw_bound=0.000, batch=289\n",
      "batch 34: loss=1.205, rw_mean=0.014, rw_bound=0.000, batch=316\n",
      "batch 35: loss=1.200, rw_mean=0.011, rw_bound=0.000, batch=338\n",
      "batch 36: loss=1.195, rw_mean=0.011, rw_bound=0.000, batch=359\n",
      "batch 37: loss=1.189, rw_mean=0.012, rw_bound=0.000, batch=381\n",
      "batch 38: loss=1.183, rw_mean=0.011, rw_bound=0.000, batch=403\n",
      "batch 39: loss=1.179, rw_mean=0.010, rw_bound=0.014, batch=422\n",
      "batch 40: loss=1.170, rw_mean=0.009, rw_bound=0.098, batch=433\n",
      "batch 41: loss=1.150, rw_mean=0.015, rw_bound=0.150, batch=438\n",
      "batch 42: loss=1.127, rw_mean=0.020, rw_bound=0.206, batch=441\n",
      "batch 43: loss=1.113, rw_mean=0.010, rw_bound=0.229, batch=432\n",
      "batch 44: loss=1.095, rw_mean=0.020, rw_bound=0.254, batch=438\n",
      "batch 45: loss=1.077, rw_mean=0.017, rw_bound=0.282, batch=429\n",
      "batch 46: loss=1.046, rw_mean=0.024, rw_bound=0.314, batch=400\n",
      "batch 47: loss=1.039, rw_mean=0.014, rw_bound=0.274, batch=420\n",
      "batch 48: loss=1.003, rw_mean=0.018, rw_bound=0.349, batch=375\n",
      "batch 49: loss=1.004, rw_mean=0.015, rw_bound=0.098, batch=402\n",
      "batch 50: loss=0.990, rw_mean=0.020, rw_bound=0.324, batch=421\n",
      "batch 51: loss=0.980, rw_mean=0.018, rw_bound=0.349, batch=433\n",
      "batch 52: loss=0.936, rw_mean=0.025, rw_bound=0.387, batch=380\n",
      "batch 53: loss=0.926, rw_mean=0.024, rw_bound=0.387, batch=404\n",
      "batch 54: loss=0.872, rw_mean=0.026, rw_bound=0.430, batch=322\n",
      "batch 55: loss=0.882, rw_mean=0.024, rw_bound=0.000, batch=363\n",
      "batch 56: loss=0.869, rw_mean=0.026, rw_bound=0.314, batch=388\n",
      "batch 57: loss=0.846, rw_mean=0.028, rw_bound=0.387, batch=400\n",
      "batch 58: loss=0.830, rw_mean=0.030, rw_bound=0.430, batch=411\n",
      "batch 59: loss=0.758, rw_mean=0.029, rw_bound=0.478, batch=298\n",
      "batch 60: loss=0.798, rw_mean=0.029, rw_bound=0.019, batch=348\n",
      "batch 61: loss=0.758, rw_mean=0.038, rw_bound=0.387, batch=371\n",
      "batch 62: loss=0.749, rw_mean=0.034, rw_bound=0.430, batch=395\n",
      "batch 63: loss=0.714, rw_mean=0.033, rw_bound=0.478, batch=374\n",
      "batch 64: loss=0.613, rw_mean=0.045, rw_bound=0.531, batch=220\n",
      "batch 65: loss=0.682, rw_mean=0.042, rw_bound=0.000, batch=286\n",
      "batch 66: loss=0.660, rw_mean=0.047, rw_bound=0.387, batch=327\n",
      "batch 67: loss=0.659, rw_mean=0.032, rw_bound=0.387, batch=364\n",
      "batch 68: loss=0.621, rw_mean=0.052, rw_bound=0.430, batch=383\n",
      "batch 69: loss=0.586, rw_mean=0.037, rw_bound=0.478, batch=378\n",
      "batch 70: loss=0.536, rw_mean=0.041, rw_bound=0.531, batch=338\n",
      "batch 71: loss=0.524, rw_mean=0.048, rw_bound=0.531, batch=366\n",
      "batch 72: loss=0.513, rw_mean=0.048, rw_bound=0.531, batch=387\n",
      "batch 74: loss=0.672, rw_mean=0.053, rw_bound=0.000, batch=72\n",
      "batch 75: loss=0.715, rw_mean=0.052, rw_bound=0.000, batch=145\n",
      "batch 76: loss=0.731, rw_mean=0.045, rw_bound=0.000, batch=211\n",
      "batch 77: loss=0.701, rw_mean=0.060, rw_bound=0.349, batch=286\n",
      "batch 78: loss=0.661, rw_mean=0.057, rw_bound=0.387, batch=337\n",
      "batch 79: loss=0.613, rw_mean=0.057, rw_bound=0.430, batch=369\n",
      "batch 80: loss=0.546, rw_mean=0.063, rw_bound=0.478, batch=360\n",
      "batch 81: loss=0.446, rw_mean=0.069, rw_bound=0.531, batch=265\n",
      "batch 82: loss=0.438, rw_mean=0.068, rw_bound=0.531, batch=311\n",
      "batch 84: loss=0.613, rw_mean=0.062, rw_bound=0.000, batch=83\n",
      "batch 85: loss=0.616, rw_mean=0.072, rw_bound=0.000, batch=179\n",
      "batch 86: loss=0.551, rw_mean=0.074, rw_bound=0.430, batch=246\n",
      "batch 87: loss=0.493, rw_mean=0.081, rw_bound=0.478, batch=291\n",
      "batch 88: loss=0.404, rw_mean=0.078, rw_bound=0.531, batch=258\n",
      "batch 90: loss=0.579, rw_mean=0.080, rw_bound=0.000, batch=107\n",
      "batch 91: loss=0.596, rw_mean=0.077, rw_bound=0.000, batch=211\n",
      "batch 92: loss=0.537, rw_mean=0.076, rw_bound=0.430, batch=279\n",
      "batch 93: loss=0.466, rw_mean=0.086, rw_bound=0.478, batch=318\n",
      "batch 94: loss=0.380, rw_mean=0.085, rw_bound=0.531, batch=278\n",
      "batch 96: loss=0.572, rw_mean=0.084, rw_bound=0.000, batch=112\n",
      "batch 97: loss=0.496, rw_mean=0.098, rw_bound=0.430, batch=216\n",
      "batch 98: loss=0.432, rw_mean=0.089, rw_bound=0.478, batch=278\n",
      "batch 99: loss=0.363, rw_mean=0.098, rw_bound=0.531, batch=284\n",
      "batch 100: loss=0.358, rw_mean=0.080, rw_bound=0.531, batch=337\n",
      "batch 102: loss=0.507, rw_mean=0.099, rw_bound=0.000, batch=129\n",
      "batch 103: loss=0.422, rw_mean=0.102, rw_bound=0.478, batch=217\n",
      "batch 104: loss=0.342, rw_mean=0.100, rw_bound=0.531, batch=241\n",
      "batch 106: loss=0.478, rw_mean=0.095, rw_bound=0.000, batch=122\n",
      "batch 107: loss=0.388, rw_mean=0.106, rw_bound=0.478, batch=219\n",
      "batch 108: loss=0.326, rw_mean=0.098, rw_bound=0.531, batch=255\n",
      "batch 110: loss=0.498, rw_mean=0.094, rw_bound=0.000, batch=123\n",
      "batch 111: loss=0.395, rw_mean=0.106, rw_bound=0.478, batch=219\n",
      "batch 112: loss=0.314, rw_mean=0.105, rw_bound=0.531, batch=249\n",
      "batch 114: loss=0.462, rw_mean=0.101, rw_bound=0.000, batch=130\n",
      "batch 115: loss=0.367, rw_mean=0.097, rw_bound=0.478, batch=228\n",
      "batch 116: loss=0.303, rw_mean=0.098, rw_bound=0.531, batch=263\n",
      "batch 118: loss=0.383, rw_mean=0.117, rw_bound=0.478, batch=130\n",
      "batch 119: loss=0.299, rw_mean=0.113, rw_bound=0.531, batch=203\n",
      "batch 121: loss=0.430, rw_mean=0.105, rw_bound=0.000, batch=134\n",
      "batch 122: loss=0.286, rw_mean=0.113, rw_bound=0.531, batch=202\n",
      "batch 124: loss=0.340, rw_mean=0.119, rw_bound=0.478, batch=135\n",
      "batch 125: loss=0.281, rw_mean=0.113, rw_bound=0.531, batch=214\n",
      "batch 127: loss=0.436, rw_mean=0.110, rw_bound=0.395, batch=140\n",
      "batch 128: loss=0.280, rw_mean=0.125, rw_bound=0.531, batch=219\n",
      "batch 130: loss=0.341, rw_mean=0.118, rw_bound=0.478, batch=132\n",
      "batch 131: loss=0.274, rw_mean=0.115, rw_bound=0.531, batch=221\n",
      "batch 133: loss=0.324, rw_mean=0.124, rw_bound=0.515, batch=140\n",
      "batch 134: loss=0.266, rw_mean=0.124, rw_bound=0.531, batch=235\n",
      "batch 136: loss=0.260, rw_mean=0.124, rw_bound=0.531, batch=118\n",
      "batch 138: loss=0.257, rw_mean=0.126, rw_bound=0.531, batch=130\n",
      "batch 140: loss=0.253, rw_mean=0.133, rw_bound=0.531, batch=137\n",
      "batch 142: loss=0.256, rw_mean=0.123, rw_bound=0.531, batch=123\n",
      "batch 144: loss=0.246, rw_mean=0.127, rw_bound=0.531, batch=129\n",
      "batch 146: loss=0.243, rw_mean=0.131, rw_bound=0.531, batch=132\n",
      "batch 148: loss=0.249, rw_mean=0.131, rw_bound=0.531, batch=132\n",
      "batch 150: loss=0.247, rw_mean=0.124, rw_bound=0.531, batch=136\n",
      "batch 152: loss=0.244, rw_mean=0.136, rw_bound=0.531, batch=134\n",
      "batch 154: loss=0.243, rw_mean=0.131, rw_bound=0.531, batch=130\n",
      "batch 157: loss=0.233, rw_mean=0.130, rw_bound=0.531, batch=136\n",
      "batch 161: loss=0.227, rw_mean=0.134, rw_bound=0.573, batch=140\n",
      "batch 163: loss=0.233, rw_mean=0.130, rw_bound=0.531, batch=135\n",
      "batch 168: loss=0.230, rw_mean=0.133, rw_bound=0.531, batch=137\n",
      "batch 173: loss=0.228, rw_mean=0.133, rw_bound=0.531, batch=131\n",
      "batch 175: loss=0.234, rw_mean=0.131, rw_bound=0.573, batch=140\n",
      "batch 178: loss=0.224, rw_mean=0.127, rw_bound=0.531, batch=131\n",
      "batch 183: loss=0.214, rw_mean=0.135, rw_bound=0.573, batch=140\n",
      "batch 189: loss=0.224, rw_mean=0.135, rw_bound=0.573, batch=140\n",
      "batch 224: loss=0.226, rw_mean=0.130, rw_bound=0.531, batch=134\n",
      "batch 237: loss=0.213, rw_mean=0.129, rw_bound=0.531, batch=139\n",
      "batch 271: loss=0.213, rw_mean=0.136, rw_bound=0.531, batch=139\n",
      "batch 305: loss=0.217, rw_mean=0.132, rw_bound=0.531, batch=139\n",
      "batch 606: loss=0.223, rw_mean=0.129, rw_bound=0.531, batch=133\n",
      "batch 1094: loss=0.203, rw_mean=0.135, rw_bound=0.573, batch=140\n",
      "batch 1125: loss=0.210, rw_mean=0.129, rw_bound=0.531, batch=136\n",
      "batch 2129: loss=0.208, rw_mean=0.134, rw_bound=0.531, batch=139\n",
      "batch 4177: loss=0.201, rw_mean=0.128, rw_bound=0.531, batch=135\n",
      "batch 4718: loss=0.209, rw_mean=0.134, rw_bound=0.531, batch=139\n",
      "batch 5163: loss=0.202, rw_mean=0.136, rw_bound=0.573, batch=140\n"
     ]
    }
   ],
   "source": [
    "full_batch = []\n",
    "\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    reward_mean = float(np.mean(list(map(lambda s: np.mean(s.reward), batch))))\n",
    "    # 保存了好的情况下的批次\n",
    "    full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
    "    if not full_batch:\n",
    "        continue\n",
    "    obs_v = torch.FloatTensor(np.array(obs))\n",
    "    acts_v = torch.LongTensor(np.array(acts))\n",
    "    full_batch = full_batch[-500:]\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if iter_no % 50 == 0 and iter_no != 0:\n",
    "    print(\n",
    "        \"batch %d: loss=%.3f, rw_mean=%.3f, \"\n",
    "        \"rw_bound=%.3f, batch=%d\"\n",
    "            % (iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch))\n",
    "    )\n",
    "    if reward_mean > 0.8:\n",
    "        print(\"Solved!\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
