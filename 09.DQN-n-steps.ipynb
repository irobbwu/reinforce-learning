{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61bb43f-d5be-497c-b302-a7a76a6e9049",
   "metadata": {},
   "source": [
    "# 09.DQN-n-steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665075ac-1cde-46de-b2b2-de898f685c4c",
   "metadata": {},
   "source": [
    "1）使用随机权重$（w←1.0）$初始化目标网络$Q(s, a, w)$和网络$\\hat Q(s, a, w)$，$Q$和$\\hat Q$相同，清空回放缓冲区。\n",
    "\n",
    "2）以概率ε选择一个随机动作a，否则 $a=argmaxQ(s,a,w)$。\n",
    "\n",
    "3）在模拟器中执行动作a，观察奖励r和下一个状态s'。\n",
    "\n",
    "4）将转移过程(s, a, r, s')存储在回放缓冲区中 r 用 n 步合计展示。\n",
    "\n",
    "5）从回放缓冲区中采样一个随机的小批量转移过程。\n",
    "\n",
    "6）对于回放缓冲区中的每个转移过程，如果片段在此步结束，则计算目标$y=r$，否则计算$y=r+\\gamma max \\hat Q(s, a, w)$ 。\n",
    "\n",
    "7）计算损失：$L=(Q(s, a, w)–y)^2$。\n",
    "\n",
    "8）固定网络$\\hat Q(s, a, w)$不变，通过最小化模型参数的损失，使用SGD算法更新$Q(s, a)$。\n",
    "\n",
    "9）每N步，将权重从目标网络$Q$复制到$\\hat Q(s, a, w)$ 。\n",
    "\n",
    "10）从步骤2开始重复，直到收敛为止。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6af8a5-f3db-452e-aca1-4415b6053543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gym.envs.toy_text import frozen_lake\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c325c7f-8676-4b7e-84f2-13c5dcccc175",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, q_table_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            # 输入为状态，样本为（1*n）\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(hidden_size, q_table_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n,)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "def discount_reward(r_history, gamma):\n",
    "    n = len(r_history)\n",
    "    dr = 0\n",
    "    for i in range(n):\n",
    "        dr += gamma**i * r_history[i]\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1ec6c-2405-4b4d-be91-61577e8fc211",
   "metadata": {},
   "source": [
    "# ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2face0-8a57-4e6a-af31-5378d5cb8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, episode_size, replay_time):\n",
    "        # 存取 queue episode\n",
    "        self.queue = []\n",
    "        self.queue_size = episode_size\n",
    "        self.replay_time = replay_time\n",
    "\n",
    "    def get_batch_queue(self, env, action_trigger, batch_size, epsilon):\n",
    "        def insert_sample_to_queue(env):\n",
    "            state, info = env.reset()\n",
    "            stop = 0\n",
    "            episode = []\n",
    "\n",
    "            while True:\n",
    "                if np.random.uniform(0, 1, 1) > epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = action_trigger(state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                episode.append([state, action, next_state, reward, terminated])\n",
    "                state = next_state\n",
    "                if terminated:\n",
    "                    state, info = env.reset()\n",
    "                    self.queue.append(episode)\n",
    "                    episode = []\n",
    "                    stop += 1\n",
    "                    continue\n",
    "                if stop >= replay_time:\n",
    "                    self.queue.append(episode)\n",
    "                    episode = []\n",
    "                    break\n",
    "\n",
    "        def init_queue(env):\n",
    "            while True:\n",
    "                insert_sample_to_queue(env)\n",
    "                if len(self.queue) >= self.queue_size:\n",
    "                    break\n",
    "\n",
    "        init_queue(env)\n",
    "        insert_sample_to_queue(env)\n",
    "        self.queue = self.queue[-self.queue_size :]\n",
    "\n",
    "        return random.sample(self.queue, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be83a26-89f6-4c9a-9185-1dc7203612c6",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f672a2bf-05df-418c-91a4-d661f35087b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, obs_size, hidden_size, q_table_size):\n",
    "        self.env = env\n",
    "        self.net = Net(obs_size, hidden_size, q_table_size)\n",
    "        self.tgt_net = Net(obs_size, hidden_size, q_table_size)\n",
    "\n",
    "    # 更新net参数\n",
    "    def update_net_parameters(self, update=True):\n",
    "        self.net.load_state_dict(self.tgt_net.state_dict())\n",
    "\n",
    "    def get_action_trigger(self, state):\n",
    "        state = torch.Tensor(state)\n",
    "        action = int(torch.argmax(self.tgt_net(state).detach()))\n",
    "        return action\n",
    "\n",
    "    # 计算y_hat_and_y\n",
    "    def calculate_y_hat_and_y(self, batch, gamma):\n",
    "        # n_step\n",
    "        state_space = []\n",
    "        action_spcae = []\n",
    "        y = []\n",
    "\n",
    "        for episode in batch:\n",
    "            random_n = int(np.random.uniform(0, len(episode), 1))\n",
    "            episode = episode[-random_n:]\n",
    "            state, action, next_state, reward, terminated = episode[-1]\n",
    "            q_table_net = dqn.net(torch.Tensor(next_state)).detach()\n",
    "            reward = reward + (1 - terminated) * gamma * float(torch.max(q_table_net))\n",
    "            episode[-1] = state, action, next_state, reward, terminated\n",
    "            reward_space = [_[3] for _ in episode]\n",
    "            r_n_steps = discount_reward(reward_space, gamma)\n",
    "            y.append(r_n_steps)\n",
    "            state, action, next_state, reward, terminated = episode[0]\n",
    "            state_space.append(state)\n",
    "            action_spcae.append(action)\n",
    "\n",
    "        y_hat = self.tgt_net(torch.Tensor(np.array(state_space)))\n",
    "        y_hat = y_hat.gather(1, torch.LongTensor(action_spcae).reshape(-1, 1))\n",
    "        return y_hat.reshape(-1), torch.tensor(y)\n",
    "\n",
    "    def predict_reward(self):\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        reward_space = []\n",
    "\n",
    "        while True:\n",
    "            step += 1\n",
    "            state = torch.Tensor(state)\n",
    "            action = int(torch.argmax(self.net(state).detach()))\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            reward_space.append(reward)\n",
    "            state = next_state\n",
    "            if terminated:\n",
    "                state, info = env.reset()\n",
    "                continue\n",
    "            if step >= 100:\n",
    "                break\n",
    "        return float(np.mean(reward_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c65fde-9f1d-4f36-b79b-e06a314ad5bf",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56d34d36-fe18-4790-9194-46818a4b9ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "queue_size = 500\n",
    "replay_time = 50\n",
    "\n",
    "## 初始化环境\n",
    "env = frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "env.spec = gym.spec(\"FrozenLake-v1\")\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "env = DiscreteOneHotWrapper(env)\n",
    "\n",
    "## 初始化buffer\n",
    "replay_buffer = ReplayBuffer(queue_size, replay_time)\n",
    "\n",
    "## 初始化dqn\n",
    "obs_size = env.observation_space.shape[0]\n",
    "q_table_size = env.action_space.n\n",
    "dqn = DQN(env, obs_size, hidden_size, q_table_size)\n",
    "\n",
    "# 定义优化器\n",
    "opt = optim.Adam(dqn.tgt_net.parameters(), lr=0.01)\n",
    "\n",
    "# 定义损失函数\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"logs/DQN/n_steps_FrozenLake\", comment=\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd682d3-7daf-460f-a040-4093ddc6b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epsilon = 0.8\n",
    "epochs = 200\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06f59cea-35d7-49f9-8296-4c203c10fef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,  MSE: 0.01869170367717743, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:1,  MSE: 0.0029035082552582026, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:2,  MSE: 0.011312279850244522, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:3,  MSE: 0.005949886050075293, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:4,  MSE: 0.005503702908754349, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:5,  MSE: 0.0059446669183671474, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:6,  MSE: 0.0030409502796828747, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:7,  MSE: 0.002284332411363721, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:8,  MSE: 0.002898065373301506, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:9,  MSE: 0.004358756355941296, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:10,  MSE: 0.053469061851501465, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:11,  MSE: 0.04703954607248306, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:12,  MSE: 0.05461917817592621, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:13,  MSE: 0.060953542590141296, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:14,  MSE: 0.07102614641189575, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:15,  MSE: 0.05078554153442383, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:16,  MSE: 0.06392958760261536, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:17,  MSE: 0.08368097245693207, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:18,  MSE: 0.0754479467868805, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:19,  MSE: 0.08387131243944168, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:20,  MSE: 0.07836461067199707, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:21,  MSE: 0.07251393795013428, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:22,  MSE: 0.06014270335435867, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:23,  MSE: 0.055173344910144806, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:24,  MSE: 0.05851670727133751, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:25,  MSE: 0.057558875530958176, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:26,  MSE: 0.0577823705971241, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:27,  MSE: 0.06587225198745728, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:28,  MSE: 0.059241198003292084, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:29,  MSE: 0.052081719040870667, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:30,  MSE: 0.07384772598743439, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:31,  MSE: 0.06424856930971146, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:32,  MSE: 0.05314202979207039, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:33,  MSE: 0.06686384230852127, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:34,  MSE: 0.06007062643766403, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:35,  MSE: 0.064552903175354, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:36,  MSE: 0.05665342137217522, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:37,  MSE: 0.07060273736715317, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:38,  MSE: 0.06067819148302078, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:39,  MSE: 0.06789287179708481, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:40,  MSE: 0.055460475385189056, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:41,  MSE: 0.05920267477631569, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:42,  MSE: 0.05957692116498947, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:43,  MSE: 0.059861041605472565, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:44,  MSE: 0.06328770518302917, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:45,  MSE: 0.062468670308589935, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:46,  MSE: 0.04656628891825676, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:47,  MSE: 0.0610157772898674, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:48,  MSE: 0.04149468615651131, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:49,  MSE: 0.05432279407978058, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:50,  MSE: 0.056346599012613297, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:51,  MSE: 0.06169814616441727, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:52,  MSE: 0.04988671839237213, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:53,  MSE: 0.052240632474422455, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:54,  MSE: 0.05408640205860138, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:55,  MSE: 0.055281683802604675, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:56,  MSE: 0.04459329694509506, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:57,  MSE: 0.05005917325615883, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:58,  MSE: 0.04506246745586395, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:59,  MSE: 0.11249986290931702, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:60,  MSE: 0.10640686005353928, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:61,  MSE: 0.09117026627063751, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:62,  MSE: 0.08648025989532471, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:63,  MSE: 0.07950717955827713, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:64,  MSE: 0.05900377780199051, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:65,  MSE: 0.06005633622407913, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:66,  MSE: 0.05944771692156792, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:67,  MSE: 0.04899148270487785, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:68,  MSE: 0.0441773384809494, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:69,  MSE: 0.049256861209869385, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:70,  MSE: 0.05270062014460564, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:71,  MSE: 0.0541226826608181, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:72,  MSE: 0.049157481640577316, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:73,  MSE: 0.055968981236219406, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:74,  MSE: 0.05864250659942627, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:75,  MSE: 0.05181918293237686, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:76,  MSE: 0.05326450243592262, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:77,  MSE: 0.060813695192337036, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:78,  MSE: 0.091411292552948, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:79,  MSE: 0.07613308727741241, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:80,  MSE: 0.06344050914049149, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:81,  MSE: 0.05930820479989052, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:82,  MSE: 0.05204315111041069, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:83,  MSE: 0.05621277913451195, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:84,  MSE: 0.051936566829681396, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:85,  MSE: 0.068378746509552, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:86,  MSE: 0.06933056563138962, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:87,  MSE: 0.07026921212673187, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:88,  MSE: 0.06421658396720886, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:89,  MSE: 0.060327693819999695, epsilon: 0.8, 100 steps reward: 0.0\n",
      "epoch:90,  MSE: 0.04968184232711792, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:91,  MSE: 0.05912463739514351, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:92,  MSE: 0.063350610435009, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:93,  MSE: 0.06215810775756836, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:94,  MSE: 0.05362381041049957, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:95,  MSE: 0.059511102735996246, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:96,  MSE: 0.05931825190782547, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:97,  MSE: 0.05044317618012428, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:98,  MSE: 0.05374372750520706, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:99,  MSE: 0.055418141186237335, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:100,  MSE: 0.048998765647411346, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:101,  MSE: 0.05378853902220726, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:102,  MSE: 0.04915808513760567, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:103,  MSE: 0.05818745866417885, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:104,  MSE: 0.05950835719704628, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:105,  MSE: 0.06191990152001381, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:106,  MSE: 0.05927148088812828, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:107,  MSE: 0.04882127791643143, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:108,  MSE: 0.08539977669715881, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:109,  MSE: 0.08667702227830887, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:110,  MSE: 0.0735173225402832, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:111,  MSE: 0.06393898278474808, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:112,  MSE: 0.06615515053272247, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:113,  MSE: 0.06375652551651001, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:114,  MSE: 0.05671517550945282, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:115,  MSE: 0.06493960320949554, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:116,  MSE: 0.05133236572146416, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:117,  MSE: 0.05512571707367897, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:118,  MSE: 0.05423508211970329, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:119,  MSE: 0.05224857106804848, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:120,  MSE: 0.054205916821956635, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:121,  MSE: 0.05511194467544556, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:122,  MSE: 0.051229625940322876, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:123,  MSE: 0.05484325811266899, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:124,  MSE: 0.05704040825366974, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:125,  MSE: 0.045516930520534515, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:126,  MSE: 0.05297746881842613, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:127,  MSE: 0.048591502010822296, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:128,  MSE: 0.04927145689725876, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:129,  MSE: 0.04826509952545166, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:130,  MSE: 0.054323501884937286, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:131,  MSE: 0.0455072820186615, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:132,  MSE: 0.05098458006978035, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:133,  MSE: 0.049482084810733795, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:134,  MSE: 0.04438655078411102, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:135,  MSE: 0.05337660759687424, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:136,  MSE: 0.05916072800755501, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:137,  MSE: 0.05277654156088829, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:138,  MSE: 0.05084716156125069, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:139,  MSE: 0.048749953508377075, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:140,  MSE: 0.05037578567862511, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:141,  MSE: 0.04827795922756195, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:142,  MSE: 0.058360468596220016, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:143,  MSE: 0.05216905474662781, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:144,  MSE: 0.04221973940730095, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:145,  MSE: 0.05313313752412796, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:146,  MSE: 0.05138983204960823, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:147,  MSE: 0.052360355854034424, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:148,  MSE: 0.05953895300626755, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:149,  MSE: 0.056890446692705154, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:150,  MSE: 0.05513722449541092, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:151,  MSE: 0.04660295695066452, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:152,  MSE: 0.05307645723223686, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:153,  MSE: 0.05090890824794769, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:154,  MSE: 0.04903046041727066, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:155,  MSE: 0.04583312198519707, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:156,  MSE: 0.047585830092430115, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:157,  MSE: 0.04438113421201706, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:158,  MSE: 0.04781298339366913, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:159,  MSE: 0.04986667260527611, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:160,  MSE: 0.048238661140203476, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:161,  MSE: 0.049814898520708084, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:162,  MSE: 0.05199534818530083, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:163,  MSE: 0.050468143075704575, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:164,  MSE: 0.0428338460624218, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:165,  MSE: 0.03788106516003609, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:166,  MSE: 0.05107903108000755, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:167,  MSE: 0.05153510347008705, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:168,  MSE: 0.045392684638500214, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:169,  MSE: 0.050555791705846786, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:170,  MSE: 0.05229804292321205, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:171,  MSE: 0.054682422429323196, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:172,  MSE: 0.054054684937000275, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:173,  MSE: 0.060063041746616364, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:174,  MSE: 0.04703100025653839, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:175,  MSE: 0.05689464509487152, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:176,  MSE: 0.041720449924468994, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:177,  MSE: 0.06535576283931732, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:178,  MSE: 0.04951810464262962, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:179,  MSE: 0.056065794080495834, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:180,  MSE: 0.05168583244085312, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:181,  MSE: 0.0544140562415123, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:182,  MSE: 0.05140499770641327, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:183,  MSE: 0.04762087017297745, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:184,  MSE: 0.04702460393309593, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:185,  MSE: 0.04443925619125366, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:186,  MSE: 0.054821666330099106, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:187,  MSE: 0.052424684166908264, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:188,  MSE: 0.05599066987633705, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:189,  MSE: 0.06156260147690773, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:190,  MSE: 0.04847612977027893, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:191,  MSE: 0.042588986456394196, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:192,  MSE: 0.05720391869544983, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:193,  MSE: 0.050399407744407654, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:194,  MSE: 0.05665219575166702, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:195,  MSE: 0.04268692433834076, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:196,  MSE: 0.04634680226445198, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:197,  MSE: 0.0483846552670002, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:198,  MSE: 0.054235029965639114, epsilon: 0.8, 100 steps reward: 0.16\n",
      "epoch:199,  MSE: 0.05034475773572922, epsilon: 0.8, 100 steps reward: 0.16\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    batch = replay_buffer.get_batch_queue(\n",
    "        env, dqn.get_action_trigger, batch_size, epsilon\n",
    "    )\n",
    "    y_hat, y = dqn.calculate_y_hat_and_y(batch, gamma)\n",
    "    l = loss(y_hat, y)\n",
    "\n",
    "    # 反向传播\n",
    "    opt.zero_grad()\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        dqn.update_net_parameters()\n",
    "\n",
    "    predict_reward = dqn.predict_reward()\n",
    "    writer.add_scalars(\n",
    "        \"MSE\", {\"loss\": l.item(), \"predict_reward\": predict_reward}, epoch\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"epoch:{},  MSE: {}, epsilon: {}, 100 steps reward: {}\".format(\n",
    "            epoch, l, epsilon, predict_reward\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cccd2e9-a413-4982-b071-68aa3690d897",
   "metadata": {},
   "source": [
    "# 可视化预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82486b91-4d0f-44bd-acbc-46bc9668efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN_Q = dqn.net\n",
    "\n",
    "# env = frozen_lake.FrozenLakeEnv(is_slippery=False, render_mode=\"human\")\n",
    "# env.spec = gym.spec(\"FrozenLake-v1\")\n",
    "# # display_size = 512\n",
    "# # env.window_size = (display_size, display_size)\n",
    "# # env.cell_size = (\n",
    "# #     env.window_size[0] // env.ncol,\n",
    "# #     env.window_size[1] // env.nrow,\n",
    "# # )\n",
    "# env = gym.wrappers.RecordVideo(env, video_folder=\"video\")\n",
    "\n",
    "# env = DiscreteOneHotWrapper(env)\n",
    "\n",
    "# state, info = env.reset()\n",
    "# total_rewards = 0\n",
    "\n",
    "# while True:\n",
    "#     action = int(torch.argmax(DQN_Q(torch.Tensor(state))))\n",
    "#     state, reward, terminated, truncted, info = env.step(action)\n",
    "#     print(action)\n",
    "#     if terminated:\n",
    "#         break\n",
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
