{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd626532-e99d-40a7-be1a-b214182595a3",
   "metadata": {},
   "source": [
    "# 交叉熵方法的实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c8069-a570-4b38-abd0-cb57f26910d6",
   "metadata": {},
   "source": [
    "1）使用当前的模型和环境产生N次片段。\r\n",
    "\r\n",
    "2）计算每个片段的总奖励，并确定奖励边界。通常使用总奖励的百分位来确定，例如50或70。\r\n",
    "\r\n",
    "\r\n",
    "3）将奖励在边界之下的片段丢掉。\r\n",
    "\r\n",
    "\r\n",
    "4）用观察值作为输入、智能体产生的动作作为目标输出，训练剩余的“精英”片段。\r\n",
    "\r\n",
    "\r\n",
    "5）从第1步开始重复，直到得到满意的结果意的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dedb9f-42a5-49df-acb6-07098822dd6f",
   "metadata": {},
   "source": [
    "## 记录条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66faab4b-8fe7-4f2c-b03a-641f535d399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Episode = namedtuple(\"Episode\", field_names=[\"reward\", \"steps\"])\n",
    "EpisodeStep = namedtuple(\"EpisodeStep\", field_names=[\"observation\", \"action\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e101b15-9306-4499-8551-d71898ad0118",
   "metadata": {},
   "source": [
    "## 实验过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "867cd4aa-4872-416a-9463-3cdae93bf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67bc39eb-7b6c-4f08-9261-581dfa231300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751040ce-83f8-4f93-9d41-332099a1f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批处理\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    # 该状态的状态值\n",
    "    obs, info = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    # while True:\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor(obs.reshape(1, -1))\n",
    "        act_probs_y = sm(net(obs_v))\n",
    "        act_probs = act_probs_y.detach().numpy()[0]\n",
    "        action = np.random.choice([0, 1], p=act_probs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        ## 更新 step\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "\n",
    "        if terminated:\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs, info = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                env\n",
    "\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c2b1312-800c-4f4c-8a09-6602bc8623c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选批\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for reward, steps in batch:\n",
    "        if reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, steps))\n",
    "        train_act.extend(map(lambda step: step.action, steps))\n",
    "    train_obs_v = torch.FloatTensor(np.array(train_obs))\n",
    "    train_act_v = torch.LongTensor(np.array(train_act))\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36ee4b-74e3-484d-9603-b40e394f6474",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8314ccf-dea9-481b-a267-fecb4a08ed4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 初始化\n",
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "PERCENTILE = 80\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "# env = gym.wrappers.RecordVideo(env, video_folder=\"video\", name_prefix=\"mario\", video_length=200)\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(log_dir=\"logs/plot_4\", comment=\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352f09b0-4fbf-47de-adc1-9692142035b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# idx = 0\n",
    "# for i in range(len(batch_data)):\n",
    "#     for j in range(len(batch_data[i])):\n",
    "\n",
    "#         print(idx,(len(batch_data[i][j].steps)))\n",
    "#         idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1574f1b8-c412-41d4-a678-4f76c0a8626d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.687, reward_mean=21.2, rw_bound=30.8\n",
      "1: loss=0.586, reward_mean=14.6, rw_bound=18.0\n",
      "2: loss=0.664, reward_mean=18.5, rw_bound=24.4\n",
      "3: loss=0.662, reward_mean=28.7, rw_bound=39.0\n",
      "4: loss=0.659, reward_mean=29.2, rw_bound=40.4\n",
      "5: loss=0.627, reward_mean=31.9, rw_bound=41.8\n",
      "6: loss=0.624, reward_mean=29.4, rw_bound=38.8\n",
      "7: loss=0.614, reward_mean=43.6, rw_bound=57.4\n",
      "8: loss=0.591, reward_mean=60.0, rw_bound=83.4\n",
      "9: loss=0.599, reward_mean=64.8, rw_bound=88.4\n",
      "10: loss=0.592, reward_mean=73.9, rw_bound=101.2\n",
      "11: loss=0.582, reward_mean=77.1, rw_bound=110.6\n",
      "12: loss=0.579, reward_mean=63.5, rw_bound=78.8\n",
      "13: loss=0.575, reward_mean=71.9, rw_bound=103.6\n",
      "14: loss=0.567, reward_mean=75.5, rw_bound=104.2\n",
      "15: loss=0.556, reward_mean=87.3, rw_bound=124.6\n",
      "16: loss=0.544, reward_mean=95.3, rw_bound=144.8\n",
      "17: loss=0.525, reward_mean=95.3, rw_bound=128.8\n",
      "18: loss=0.512, reward_mean=94.0, rw_bound=129.0\n",
      "19: loss=0.520, reward_mean=103.5, rw_bound=141.8\n",
      "20: loss=0.501, reward_mean=106.5, rw_bound=132.8\n",
      "21: loss=0.503, reward_mean=117.1, rw_bound=137.8\n",
      "22: loss=0.507, reward_mean=134.7, rw_bound=168.2\n",
      "23: loss=0.502, reward_mean=151.2, rw_bound=188.4\n",
      "24: loss=0.487, reward_mean=163.8, rw_bound=211.2\n",
      "25: loss=0.493, reward_mean=165.0, rw_bound=221.6\n",
      "26: loss=0.491, reward_mean=181.0, rw_bound=238.8\n",
      "27: loss=0.479, reward_mean=210.5, rw_bound=283.6\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "batch_data = []\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    batch_data.append(batch)\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\n",
    "        \"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\"\n",
    "        % (iter_no, loss_v.item(), reward_m, reward_b)\n",
    "    )\n",
    "    writer.add_scalars(\n",
    "        \"graph\",\n",
    "        {\"loss\": loss_v.item(), \"reward_bound\": reward_b, \"reward_mean\": reward_m},\n",
    "        iter_no\n",
    "    )\n",
    "    # writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    # writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "    if reward_m > 199:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
