{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d38cb9-ef7a-492a-9e70-7700a4466607",
   "metadata": {},
   "source": [
    "# actor-critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f7405-f775-4f93-97b2-b082413c5633",
   "metadata": {},
   "source": [
    "## TD Error\n",
    "如果用$\\hat v(s,w)$代表状态值函数，则TD Error表示为\n",
    "$$r_{t+1}+\\gamma \\hat v(s_{t+1},w) -\\hat v(s_{t},w) $$\n",
    "\n",
    "令损失函数\n",
    "$$J_w = E[ v(s_{t}) -\\hat v(s_{t},w)]^2$$\n",
    "\n",
    "则利用梯度下降法最小化$J_\\theta$为\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_{k+1} =& w_k -\\alpha\\nabla_w J(w_k)\\\\\n",
    "=& w_k -\\alpha[-2E([r_{t+1}+\\gamma \\hat v(s_{t+1},w) -\\hat v(s_{t},w)])]\\nabla_w \\hat v(s_{t},w))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "用随机梯度来估算，则最小化$J_\\theta$为\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_{k+1} =& w_k -\\alpha\\nabla_w J(w_k)\\\\\n",
    "=& w_k +\\alpha[r_{t+1}+\\gamma \\hat v(s_{t+1},w) -\\hat v(s_{t},w)]\\nabla_w \\hat v(s_{t},w))\\\\\n",
    "=& w_k +\\alpha[ v(s_{t}) -\\hat v(s_{t},w)]\\nabla_w \\hat v(s_{t},w))\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "对于q—value来说，\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_{k+1} =& w_k -\\alpha\\nabla_w J(w_k)\\\\\n",
    "=& w_k +\\alpha[r_{t+1}+\\gamma \\hat q(s_{t+1}, a_{t+1},w) -\\hat q(s_{t}, a_{t},w)]\\nabla_w \\hat q(s_{t},a_{t},w))\\\\\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596ba5f-b06f-4a6d-982e-fd5bc6597919",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "\n",
    "参考上一节\n",
    "\n",
    "$$\n",
    "\\begin {align*}\n",
    "θ_{t+1} =& θ_{t} + \\nabla _{\\theta}J(θ_t)\\\\=& θ_{t} + \\nabla _{\\theta}E_{S-d,a-\\pi(S,\\Theta)}[q(s,a) \\nabla _{\\theta}ln\\pi(a|s,\\theta)]\n",
    "\\end {align*}\n",
    "$$\n",
    "一般来说，$\\nabla _{\\theta}ln\\pi(a|s,\\theta)$是未知的，可以用随机梯度法来估计，则\n",
    "$$\n",
    "\\begin {align*}\n",
    "θ_{t+1} =& θ_{t} + \\nabla _{\\theta}J(θ_t)\\\\=& θ_{t} + \\nabla _{\\theta}[q(s,a) \\nabla _{\\theta}ln\\pi(a|s,\\theta)]\n",
    "\\end {align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e3ea10-e8bb-46b8-8c27-adc58615ca8a",
   "metadata": {},
   "source": [
    "## QAC\n",
    "The simplest actor-critic algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398edb96-6afb-43f4-b336-eefb0c7049f9",
   "metadata": {},
   "source": [
    "- actor：更新策略\n",
    "  \n",
    "  我们采用reinforce的方法来更新策略函数$\\pi$，$\n",
    "\\begin {align*}\n",
    "θ_{t+1} =& θ_{t} + \\nabla _{\\theta}[q(s,a) \\nabla _{\\theta}ln\\pi(a|s,\\theta)]\n",
    "\\end {align*}\n",
    "$\n",
    "- critic：更新值\n",
    "  \n",
    "  我们采用优化td-error的方法来更新行动值$q$，$\n",
    "\\begin{align*}\n",
    "w_{k+1} =& w_k +\\alpha[r_{t+1}+\\gamma \\hat q(s_{t+1}, a_{t+1},w) -\\hat q(s_{t}, a_{t},w)]\\nabla_w \\hat q(s_{t},a_{t},w))\n",
    "\\end{align*}\n",
    "$\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d5372-7ad0-42cc-ae31-1ccd02c8a768",
   "metadata": {},
   "source": [
    "## Advantage actor-critic (A2C)\n",
    "\n",
    "\n",
    "减小方差的下一步是使基线与状态相关（这是一个好主意，因为不同的状态可能具有非常不同的基线）。确实，要决定某个特定动作在某种状态下的适用性，我们会使用该动作的折扣总奖励。但是，总奖励本身可以表示为状态的价值加上动作的优势值：Q(s,a)=V(s)+A(s,a)（参见DuelingDQN）。\n",
    "\n",
    "知道每个状态的价值（至少有一个近似值）后，我们就可以用它来计算策略梯度并更新策略网络，以增加具有良好优势值的动作的执行概率，并减少具有劣势优势值的动作的执行概率。策略网络（返回动作的概率分布）被称为行动者（actor），因为它会告诉我们该做什么。另一个网络称为评论家（critic），因为它能使我们了解自己的动作有多好。这种改进有一个众所周知的名称，即advantage actorcritic方法，通常被简称为A2C。\n",
    "$$E_{S-d,a-\\pi(S,\\Theta)}[q(s,a) \\nabla _{\\theta}ln\\pi(a|s,\\theta)]=E_{S-d,a-\\pi(S,\\Theta)}[\\nabla _{\\theta}ln\\pi(a|s,\\theta)[q(s,a) -v(s)]]$$\n",
    "\n",
    "\n",
    "- Advantage(TD error)\n",
    "\n",
    "  $\\delta_t =r_{t+1}+\\gamma v(s_{t+1};w_t)- v(s_t;w_t)$\n",
    "- actor：更新策略\n",
    "  \n",
    "  我们采用reinforce的方法来更新策略函数$\\pi$，\n",
    "\n",
    "  $\n",
    "\\begin {align*}\n",
    "θ_{t+1} =& θ_{t} + a\\delta_t\\nabla _{\\theta}[\\nabla _{\\theta}ln\\pi(a|s,\\theta)]\n",
    "\\end {align*}\n",
    "$\n",
    "- critic：更新值\n",
    "  \n",
    "  1、我们采用优化td-error的方法来更新状态值$v$，$\n",
    "\\begin{align*}\n",
    "w_{k+1} =& w_k -\\alpha\\nabla_w[ v(s_{t},w) -\\hat v(s_{t},w)]^2\n",
    "\\end{align*}$\n",
    "\n",
    "    2、在这里，使用实际发生的discount reward来估算$v(s_{t},w)$\n",
    "  \n",
    "  3、$\n",
    "\\begin{align*}\n",
    "w_{k+1} =& w_k -\\alpha\\nabla_w[R -\\hat v(s_{t},w)]^2\n",
    "\\end{align*}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70efbe7b-61fe-4e0e-8317-42014fd3e025",
   "metadata": {},
   "source": [
    "## Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce6acc7-0240-4ce8-a222-6079032fe40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.optim as optim\n",
    "from gym.envs.toy_text import frozen_lake\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28091ede-9608-48b3-bf6d-531a1fc08814",
   "metadata": {},
   "source": [
    "1. 初始化A2CNet，使其返回策略函数pi(s, a)，和价值V(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "088faf2a-11c4-4783-833d-947aa022934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CNet(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, q_table_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # 策略函数pi(s, a)\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, q_table_size),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        # 价值V(s)\n",
    "        self.v_net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        if len(torch.Tensor(state).size()) == 1:\n",
    "            state = state.reshape(1, -1)\n",
    "        return self.policy_net(state), self.v_net(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28f3a0-d2e7-4c90-aa34-24995e7f386b",
   "metadata": {},
   "source": [
    "2. 使用当前策略πθ在环境中交互N步，并保存状态（st）、动作（at）和奖励（rt）\n",
    "3. 如果片段到达结尾，则R=0，否则为Vθ(st)，这里采用环境产生的R来近似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9371d7b6-c535-423c-9a8a-87cd1765e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(R, gamma):\n",
    "    # r 为历史得分\n",
    "    n = len(R)\n",
    "    dr = 0\n",
    "    for i in range(n):\n",
    "        dr += gamma**i * R[i]\n",
    "    return dr\n",
    "\n",
    "\n",
    "def generate_episode(env, n_steps, net, gamma, predict=False):\n",
    "    episode_history = dict()\n",
    "    r_list = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        episode = []\n",
    "        predict_reward = []\n",
    "        state, info = env.reset()\n",
    "        while True:\n",
    "            p, v = net(torch.Tensor(state))\n",
    "            p = p.detach().numpy().reshape(-1)\n",
    "            action = np.random.choice(list(range(env.action_space.n)), p=p)\n",
    "            next_state, reward, terminated, truncted, info = env.step(action)\n",
    "\n",
    "            # 如果截断，则展开 v(state) = r + gamma*v(next_state)\n",
    "            if truncted and not terminated:\n",
    "                reward = reward + gamma * float(\n",
    "                    net(torch.Tensor(next_state))[1].detach()\n",
    "                )\n",
    "\n",
    "            episode.append([state, action, next_state, reward, terminated])\n",
    "            predict_reward.append(reward)\n",
    "            state = next_state\n",
    "            if terminated or truncted:\n",
    "                episode_history[_] = episode\n",
    "                r_list.append(len(episode))\n",
    "                episode = []\n",
    "                predict_reward = []\n",
    "                break\n",
    "    if predict:\n",
    "        return np.mean(r_list)\n",
    "    return episode_history\n",
    "\n",
    "\n",
    "def calculate_t_discount_reward(reward_list, gamma):\n",
    "    discount_reward = []\n",
    "    total_reward = 0\n",
    "    for i in reward_list[::-1]:\n",
    "        total_reward = total_reward * gamma + i\n",
    "        discount_reward.append(total_reward)\n",
    "    return discount_reward[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad6841-6249-4c2e-bdc9-892aa4dc4a91",
   "metadata": {},
   "source": [
    "4. 累积策略梯度$\\begin {align*}\n",
    "θ_{t+1} =& θ_{t} + a\\delta_t\\nabla _{\\theta}[\\nabla _{\\theta}ln\\pi(a|s,\\theta)]\n",
    "\\end {align*}$\n",
    "\n",
    "5. 累积价值梯度\n",
    "   $\n",
    "\\begin{align*}\n",
    "w_{k+1} =& w_k -\\alpha\\nabla_w[R -\\hat v(s_{t},w)]^2\n",
    "\\end{align*}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfdfaf24-9898-41a1-aa7f-39f847e934f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor策略损失函数\n",
    "def loss(net, batch, gamma, entropy_beta=False):\n",
    "    l = 0\n",
    "    for episode in batch.values():\n",
    "        reward_list = [\n",
    "            reward for state, action, next_state, reward, terminated in episode\n",
    "        ]\n",
    "        state = [state for state, action, next_state, reward, terminated in episode]\n",
    "        action = [action for state, action, next_state, reward, terminated in episode]\n",
    "\n",
    "        # actor策略损失函数\n",
    "        ## max entropy\n",
    "        qt = calculate_t_discount_reward(reward_list, gamma)\n",
    "        pi = net(torch.Tensor(state))[0]\n",
    "        entropy_loss = -torch.sum((pi * torch.log(pi)), axis=1).mean() * entropy_beta\n",
    "        pi = pi.gather(dim=1, index=torch.LongTensor(action).reshape(-1, 1))\n",
    "        l_policy = -torch.Tensor(qt) @ torch.log(pi)\n",
    "        if entropy_beta:\n",
    "            l_policy -= entropy_loss\n",
    "\n",
    "        # critic损失函数\n",
    "        critic_loss = nn.MSELoss()(\n",
    "            net(torch.Tensor(state))[1].reshape(-1), torch.Tensor(qt)\n",
    "        )\n",
    "        l += l_policy + critic_loss\n",
    "\n",
    "    return l / len(batch.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945a8ab-4530-4126-8e42-29b33510bec4",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d9802e9-7ac7-40b9-b486-54bfbca4f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 初始化环境\n",
    "env = gym.make(\"CartPole-v1\", max_episode_steps=200)\n",
    "# env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "state, info = env.reset()\n",
    "\n",
    "obs_n = env.observation_space.shape[0]\n",
    "hidden_num = 64\n",
    "act_n = env.action_space.n\n",
    "a2c = A2CNet(obs_n, hidden_num, act_n)\n",
    "\n",
    "# 定义优化器\n",
    "opt = optim.Adam(a2c.parameters(), lr=0.01)\n",
    "\n",
    "# 记录\n",
    "writer = SummaryWriter(log_dir=\"logs/PolicyGradient/A2C\", comment=\"test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a139fd67-3953-4e97-aec1-b392bcda6311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wu.zhengzhen\\AppData\\Local\\Temp\\ipykernel_1852\\1211702406.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  pi = net(torch.Tensor(state))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,  Loss: tensor([121.0175]), max_steps: 17.8\n",
      "epoch:1,  Loss: tensor([99.9050]), max_steps: 18.3\n",
      "epoch:2,  Loss: tensor([163.5403]), max_steps: 21.4\n",
      "epoch:3,  Loss: tensor([114.0582]), max_steps: 23.1\n",
      "epoch:4,  Loss: tensor([160.0097]), max_steps: 25.9\n",
      "epoch:5,  Loss: tensor([147.8330]), max_steps: 30.3\n",
      "epoch:6,  Loss: tensor([206.0221]), max_steps: 22.1\n",
      "epoch:7,  Loss: tensor([197.0219]), max_steps: 26.9\n",
      "epoch:8,  Loss: tensor([229.8294]), max_steps: 41.2\n",
      "epoch:9,  Loss: tensor([264.1847]), max_steps: 51.8\n",
      "epoch:10,  Loss: tensor([230.6138]), max_steps: 39.2\n",
      "epoch:11,  Loss: tensor([217.4728]), max_steps: 43.9\n",
      "epoch:12,  Loss: tensor([236.4808]), max_steps: 47.1\n",
      "epoch:13,  Loss: tensor([266.2939]), max_steps: 70.9\n",
      "epoch:14,  Loss: tensor([365.8184]), max_steps: 59.0\n",
      "epoch:15,  Loss: tensor([336.0659]), max_steps: 45.1\n",
      "epoch:16,  Loss: tensor([274.5198]), max_steps: 69.8\n",
      "epoch:17,  Loss: tensor([274.4802]), max_steps: 63.9\n",
      "epoch:18,  Loss: tensor([325.6306]), max_steps: 44.2\n",
      "epoch:19,  Loss: tensor([287.3005]), max_steps: 51.2\n",
      "epoch:20,  Loss: tensor([288.1746]), max_steps: 50.5\n",
      "epoch:21,  Loss: tensor([369.5186]), max_steps: 76.4\n",
      "epoch:22,  Loss: tensor([286.7165]), max_steps: 63.1\n",
      "epoch:23,  Loss: tensor([437.2545]), max_steps: 74.1\n",
      "epoch:24,  Loss: tensor([408.9812]), max_steps: 78.7\n",
      "epoch:25,  Loss: tensor([606.7100]), max_steps: 84.2\n",
      "epoch:26,  Loss: tensor([537.1346]), max_steps: 86.4\n",
      "epoch:27,  Loss: tensor([530.8055]), max_steps: 130.7\n",
      "epoch:28,  Loss: tensor([683.8804]), max_steps: 99.0\n",
      "epoch:29,  Loss: tensor([548.7737]), max_steps: 102.4\n",
      "epoch:30,  Loss: tensor([470.4134]), max_steps: 105.1\n",
      "epoch:31,  Loss: tensor([465.8736]), max_steps: 116.8\n",
      "epoch:32,  Loss: tensor([574.0027]), max_steps: 109.5\n",
      "epoch:33,  Loss: tensor([635.5327]), max_steps: 119.0\n",
      "epoch:34,  Loss: tensor([671.1187]), max_steps: 100.7\n",
      "epoch:35,  Loss: tensor([639.9340]), max_steps: 132.7\n",
      "epoch:36,  Loss: tensor([617.4158]), max_steps: 138.0\n",
      "epoch:37,  Loss: tensor([560.9877]), max_steps: 96.1\n",
      "epoch:38,  Loss: tensor([606.5826]), max_steps: 120.6\n",
      "epoch:39,  Loss: tensor([684.6360]), max_steps: 121.8\n",
      "epoch:40,  Loss: tensor([706.0712]), max_steps: 99.1\n",
      "epoch:41,  Loss: tensor([663.1453]), max_steps: 98.4\n",
      "epoch:42,  Loss: tensor([544.9002]), max_steps: 110.0\n",
      "epoch:43,  Loss: tensor([588.3968]), max_steps: 86.7\n",
      "epoch:44,  Loss: tensor([413.0125]), max_steps: 94.3\n",
      "epoch:45,  Loss: tensor([587.1326]), max_steps: 98.4\n",
      "epoch:46,  Loss: tensor([576.2973]), max_steps: 120.2\n",
      "epoch:47,  Loss: tensor([580.0219]), max_steps: 104.1\n",
      "epoch:48,  Loss: tensor([652.6616]), max_steps: 144.7\n",
      "epoch:49,  Loss: tensor([715.3912]), max_steps: 131.8\n",
      "epoch:50,  Loss: tensor([862.1263]), max_steps: 183.0\n",
      "epoch:51,  Loss: tensor([936.4473]), max_steps: 178.8\n",
      "epoch:52,  Loss: tensor([966.5510]), max_steps: 188.2\n",
      "epoch:53,  Loss: tensor([1014.9573]), max_steps: 186.5\n",
      "epoch:54,  Loss: tensor([931.1526]), max_steps: 184.1\n",
      "epoch:55,  Loss: tensor([1043.1559]), max_steps: 185.2\n",
      "epoch:56,  Loss: tensor([996.1510]), max_steps: 184.5\n",
      "epoch:57,  Loss: tensor([1073.6216]), max_steps: 196.2\n",
      "epoch:58,  Loss: tensor([1045.8441]), max_steps: 198.0\n",
      "epoch:59,  Loss: tensor([1058.4591]), max_steps: 200.0\n",
      "epoch:60,  Loss: tensor([1030.1448]), max_steps: 191.4\n",
      "epoch:61,  Loss: tensor([1022.7876]), max_steps: 198.6\n",
      "epoch:62,  Loss: tensor([1041.5374]), max_steps: 195.0\n",
      "epoch:63,  Loss: tensor([1018.0721]), max_steps: 184.3\n",
      "epoch:64,  Loss: tensor([1017.8412]), max_steps: 199.0\n",
      "epoch:65,  Loss: tensor([1003.6371]), max_steps: 196.5\n",
      "epoch:66,  Loss: tensor([1031.7097]), max_steps: 184.5\n",
      "epoch:67,  Loss: tensor([992.1530]), max_steps: 187.7\n",
      "epoch:68,  Loss: tensor([1010.9041]), max_steps: 181.6\n",
      "epoch:69,  Loss: tensor([1013.7079]), max_steps: 195.2\n",
      "epoch:70,  Loss: tensor([1022.1340]), max_steps: 190.1\n",
      "epoch:71,  Loss: tensor([986.5980]), max_steps: 181.4\n",
      "epoch:72,  Loss: tensor([1028.5565]), max_steps: 190.1\n",
      "epoch:73,  Loss: tensor([994.3149]), max_steps: 197.2\n",
      "epoch:74,  Loss: tensor([1036.3033]), max_steps: 193.5\n",
      "epoch:75,  Loss: tensor([1032.5366]), max_steps: 195.4\n",
      "epoch:76,  Loss: tensor([1058.8464]), max_steps: 199.2\n",
      "epoch:77,  Loss: tensor([1047.1499]), max_steps: 183.6\n",
      "epoch:78,  Loss: tensor([1012.7660]), max_steps: 192.5\n",
      "epoch:79,  Loss: tensor([1020.4620]), max_steps: 175.4\n",
      "epoch:80,  Loss: tensor([995.8617]), max_steps: 170.1\n",
      "epoch:81,  Loss: tensor([951.3914]), max_steps: 167.7\n",
      "epoch:82,  Loss: tensor([904.2673]), max_steps: 165.7\n",
      "epoch:83,  Loss: tensor([854.3151]), max_steps: 162.8\n",
      "epoch:84,  Loss: tensor([902.4836]), max_steps: 181.0\n",
      "epoch:85,  Loss: tensor([868.2864]), max_steps: 168.3\n",
      "epoch:86,  Loss: tensor([873.1378]), max_steps: 166.5\n",
      "epoch:87,  Loss: tensor([869.3291]), max_steps: 156.3\n",
      "epoch:88,  Loss: tensor([859.7500]), max_steps: 148.6\n",
      "epoch:89,  Loss: tensor([842.6862]), max_steps: 160.3\n",
      "epoch:90,  Loss: tensor([794.5792]), max_steps: 162.2\n",
      "epoch:91,  Loss: tensor([793.4418]), max_steps: 155.3\n",
      "epoch:92,  Loss: tensor([834.2611]), max_steps: 157.5\n",
      "epoch:93,  Loss: tensor([789.2167]), max_steps: 162.3\n",
      "epoch:94,  Loss: tensor([822.5878]), max_steps: 166.0\n",
      "epoch:95,  Loss: tensor([822.9974]), max_steps: 178.4\n",
      "epoch:96,  Loss: tensor([893.7532]), max_steps: 188.1\n",
      "epoch:97,  Loss: tensor([906.3705]), max_steps: 187.8\n",
      "epoch:98,  Loss: tensor([1004.5778]), max_steps: 191.4\n",
      "epoch:99,  Loss: tensor([996.5134]), max_steps: 190.6\n",
      "epoch:100,  Loss: tensor([1007.2891]), max_steps: 194.5\n",
      "epoch:101,  Loss: tensor([1004.9757]), max_steps: 191.0\n",
      "epoch:102,  Loss: tensor([1055.4348]), max_steps: 200.0\n",
      "epoch:103,  Loss: tensor([1054.2881]), max_steps: 200.0\n",
      "epoch:104,  Loss: tensor([1082.0186]), max_steps: 198.4\n",
      "epoch:105,  Loss: tensor([1091.2366]), max_steps: 200.0\n",
      "epoch:106,  Loss: tensor([1079.3264]), max_steps: 200.0\n",
      "epoch:107,  Loss: tensor([1081.8779]), max_steps: 200.0\n",
      "epoch:108,  Loss: tensor([1099.2322]), max_steps: 200.0\n",
      "epoch:109,  Loss: tensor([1092.5337]), max_steps: 200.0\n",
      "epoch:110,  Loss: tensor([1092.9589]), max_steps: 200.0\n",
      "epoch:111,  Loss: tensor([1097.2467]), max_steps: 200.0\n",
      "epoch:112,  Loss: tensor([1086.9769]), max_steps: 200.0\n",
      "epoch:113,  Loss: tensor([1097.3459]), max_steps: 200.0\n",
      "epoch:114,  Loss: tensor([1092.0525]), max_steps: 200.0\n",
      "epoch:115,  Loss: tensor([1082.7657]), max_steps: 200.0\n",
      "epoch:116,  Loss: tensor([1076.5394]), max_steps: 200.0\n",
      "epoch:117,  Loss: tensor([1077.6663]), max_steps: 200.0\n",
      "epoch:118,  Loss: tensor([1086.8718]), max_steps: 200.0\n",
      "epoch:119,  Loss: tensor([1062.5647]), max_steps: 200.0\n",
      "epoch:120,  Loss: tensor([1065.7434]), max_steps: 200.0\n",
      "epoch:121,  Loss: tensor([1069.7345]), max_steps: 200.0\n",
      "epoch:122,  Loss: tensor([1062.2247]), max_steps: 200.0\n",
      "epoch:123,  Loss: tensor([1064.3047]), max_steps: 200.0\n",
      "epoch:124,  Loss: tensor([1071.6641]), max_steps: 197.2\n",
      "epoch:125,  Loss: tensor([1039.6377]), max_steps: 199.6\n",
      "epoch:126,  Loss: tensor([1048.1584]), max_steps: 195.3\n",
      "epoch:127,  Loss: tensor([1041.9270]), max_steps: 190.2\n",
      "epoch:128,  Loss: tensor([1038.3328]), max_steps: 188.9\n",
      "epoch:129,  Loss: tensor([1068.0309]), max_steps: 195.9\n",
      "epoch:130,  Loss: tensor([1030.0583]), max_steps: 198.4\n",
      "epoch:131,  Loss: tensor([1033.8842]), max_steps: 195.7\n",
      "epoch:132,  Loss: tensor([1024.4382]), max_steps: 183.3\n",
      "epoch:133,  Loss: tensor([1047.4877]), max_steps: 189.3\n",
      "epoch:134,  Loss: tensor([1021.7816]), max_steps: 197.9\n",
      "epoch:135,  Loss: tensor([1027.8676]), max_steps: 194.4\n",
      "epoch:136,  Loss: tensor([1034.7972]), max_steps: 195.3\n",
      "epoch:137,  Loss: tensor([1011.3236]), max_steps: 196.2\n",
      "epoch:138,  Loss: tensor([1001.9097]), max_steps: 188.5\n",
      "epoch:139,  Loss: tensor([1004.5026]), max_steps: 180.2\n",
      "epoch:140,  Loss: tensor([960.1136]), max_steps: 189.0\n",
      "epoch:141,  Loss: tensor([967.9521]), max_steps: 191.6\n",
      "epoch:142,  Loss: tensor([1013.0371]), max_steps: 189.4\n",
      "epoch:143,  Loss: tensor([1030.7819]), max_steps: 197.4\n",
      "epoch:144,  Loss: tensor([1060.2020]), max_steps: 200.0\n",
      "epoch:145,  Loss: tensor([1053.8411]), max_steps: 200.0\n",
      "epoch:146,  Loss: tensor([1059.9210]), max_steps: 200.0\n",
      "epoch:147,  Loss: tensor([1057.7876]), max_steps: 200.0\n",
      "epoch:148,  Loss: tensor([1064.2659]), max_steps: 200.0\n",
      "epoch:149,  Loss: tensor([1057.1006]), max_steps: 200.0\n",
      "epoch:150,  Loss: tensor([1061.5514]), max_steps: 200.0\n",
      "epoch:151,  Loss: tensor([1062.3191]), max_steps: 200.0\n",
      "epoch:152,  Loss: tensor([1080.6108]), max_steps: 200.0\n",
      "epoch:153,  Loss: tensor([1083.5884]), max_steps: 200.0\n",
      "epoch:154,  Loss: tensor([1074.5549]), max_steps: 200.0\n",
      "epoch:155,  Loss: tensor([1074.4734]), max_steps: 200.0\n",
      "epoch:156,  Loss: tensor([1080.8389]), max_steps: 200.0\n",
      "epoch:157,  Loss: tensor([1093.8021]), max_steps: 200.0\n",
      "epoch:158,  Loss: tensor([1079.9457]), max_steps: 200.0\n",
      "epoch:159,  Loss: tensor([1074.0310]), max_steps: 200.0\n",
      "epoch:160,  Loss: tensor([1088.8689]), max_steps: 200.0\n",
      "epoch:161,  Loss: tensor([1068.7542]), max_steps: 200.0\n",
      "epoch:162,  Loss: tensor([1072.6475]), max_steps: 200.0\n",
      "epoch:163,  Loss: tensor([1061.1643]), max_steps: 200.0\n",
      "epoch:164,  Loss: tensor([1063.6560]), max_steps: 192.4\n",
      "epoch:165,  Loss: tensor([1069.9020]), max_steps: 200.0\n",
      "epoch:166,  Loss: tensor([1058.5011]), max_steps: 200.0\n",
      "epoch:167,  Loss: tensor([1065.4227]), max_steps: 200.0\n",
      "epoch:168,  Loss: tensor([1050.4962]), max_steps: 200.0\n",
      "epoch:169,  Loss: tensor([1069.1176]), max_steps: 200.0\n",
      "epoch:170,  Loss: tensor([1048.8318]), max_steps: 200.0\n",
      "epoch:171,  Loss: tensor([1060.5468]), max_steps: 200.0\n",
      "epoch:172,  Loss: tensor([1053.8733]), max_steps: 200.0\n",
      "epoch:173,  Loss: tensor([1063.3674]), max_steps: 200.0\n",
      "epoch:174,  Loss: tensor([1049.0009]), max_steps: 200.0\n",
      "epoch:175,  Loss: tensor([1051.5199]), max_steps: 200.0\n",
      "epoch:176,  Loss: tensor([1057.7239]), max_steps: 200.0\n",
      "epoch:177,  Loss: tensor([1036.6875]), max_steps: 200.0\n",
      "epoch:178,  Loss: tensor([1044.3518]), max_steps: 200.0\n",
      "epoch:179,  Loss: tensor([1046.5887]), max_steps: 200.0\n",
      "epoch:180,  Loss: tensor([1041.9917]), max_steps: 200.0\n",
      "epoch:181,  Loss: tensor([1052.9374]), max_steps: 200.0\n",
      "epoch:182,  Loss: tensor([1044.9182]), max_steps: 200.0\n",
      "epoch:183,  Loss: tensor([1040.5813]), max_steps: 200.0\n",
      "epoch:184,  Loss: tensor([1026.4171]), max_steps: 200.0\n",
      "epoch:185,  Loss: tensor([1037.9011]), max_steps: 200.0\n",
      "epoch:186,  Loss: tensor([1033.2668]), max_steps: 200.0\n",
      "epoch:187,  Loss: tensor([1044.1956]), max_steps: 200.0\n",
      "epoch:188,  Loss: tensor([1032.1057]), max_steps: 200.0\n",
      "epoch:189,  Loss: tensor([1031.1393]), max_steps: 200.0\n",
      "epoch:190,  Loss: tensor([1028.5912]), max_steps: 200.0\n",
      "epoch:191,  Loss: tensor([1035.8657]), max_steps: 200.0\n",
      "epoch:192,  Loss: tensor([1021.3992]), max_steps: 200.0\n",
      "epoch:193,  Loss: tensor([1027.3817]), max_steps: 200.0\n",
      "epoch:194,  Loss: tensor([1015.3945]), max_steps: 200.0\n",
      "epoch:195,  Loss: tensor([970.4677]), max_steps: 200.0\n",
      "epoch:196,  Loss: tensor([1036.1049]), max_steps: 200.0\n",
      "epoch:197,  Loss: tensor([1002.8406]), max_steps: 186.9\n",
      "epoch:198,  Loss: tensor([991.7887]), max_steps: 200.0\n",
      "epoch:199,  Loss: tensor([1034.4620]), max_steps: 200.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 20\n",
    "gamma = 0.9\n",
    "entropy_beta = 0.01\n",
    "# 避免梯度太大\n",
    "CLIP_GRAD = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch = generate_episode(env, batch_size, a2c, gamma)\n",
    "    l = loss(a2c, batch, gamma, entropy_beta)\n",
    "\n",
    "    # 反向传播\n",
    "    opt.zero_grad()\n",
    "    l.backward()\n",
    "    # 梯度裁剪\n",
    "    nn_utils.clip_grad_norm_(a2c.parameters(), CLIP_GRAD)\n",
    "    opt.step()\n",
    "\n",
    "    max_steps = generate_episode(env, 10, a2c, gamma, predict=True)\n",
    "    writer.add_scalars(\n",
    "        \"Loss\",\n",
    "        {\"loss\": l.item(), \"max_steps\": max_steps},\n",
    "        epoch,\n",
    "    )\n",
    "\n",
    "    print(\"epoch:{},  Loss: {}, max_steps: {}\".format(epoch, l.detach(), max_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e429e414-f88f-4625-8016-400ca0b06e8b",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f038155-9574-478d-8f9b-1b5d3c52a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "# env = gym.wrappers.RecordVideo(env, video_folder=\"video\")\n",
    "\n",
    "# state, info = env.reset()\n",
    "# total_rewards = 0\n",
    "\n",
    "# while True:\n",
    "#     p = net(torch.Tensor(state)).detach().numpy().reshape(-1)\n",
    "#     action = np.random.choice(list(range(env.action_space.n)), p=p)\n",
    "#     state, reward, terminated, truncted, info = env.step(action)\n",
    "#     if terminated:\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
