{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61bb43f-d5be-497c-b302-a7a76a6e9049",
   "metadata": {},
   "source": [
    "# 05.q_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6af8a5-f3db-452e-aca1-4415b6053543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from gym.envs.toy_text import frozen_lake\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "660e19f5-6543-4b8c-a736-b60a0bd89273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state, info = self.env.reset()\n",
    "\n",
    "        # 状态，行为，是否终止保存\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.terminates = collections.defaultdict(bool)\n",
    "\n",
    "        # q_value,state_value保存\n",
    "        self.q_table = collections.defaultdict(float)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    ## 随机模拟实验\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, terminited, truncted, info = self.env.step(action)\n",
    "            # print(self.state, action, new_state)\n",
    "            self.terminates[(self.state, action, new_state)] = terminited\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            if terminited:\n",
    "                self.state, info = self.env.reset()\n",
    "            else:\n",
    "                self.state = new_state\n",
    "\n",
    "    ## 计算 行动值\n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            reward = self.rewards[(state, action, tgt_state)]\n",
    "            val = reward + GAMMA * self.values[tgt_state]\n",
    "            action_value += (count / total) * val\n",
    "        return action_value\n",
    "\n",
    "    def q_table_update(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            for action in range(self.env.action_space.n):\n",
    "                self.q_table[(state, action)] = self.calc_action_value(state, action)\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            self.values[state] = max(\n",
    "                [\n",
    "                    self.q_table[(state, action)]\n",
    "                    for action in range(self.env.action_space.n)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def select_action(self, state):\n",
    "        return int(\n",
    "            np.argmax(\n",
    "                [\n",
    "                    self.q_table[(state, action)]\n",
    "                    for action in range(self.env.action_space.n)\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def episode(self, n, render=True):\n",
    "        # 初始状态\n",
    "        self.play_n_random_steps(200)\n",
    "        env = self.env\n",
    "\n",
    "        # 可视化\n",
    "        if render:\n",
    "            env.render_mode = \"human\"\n",
    "            display_size = 512\n",
    "            env.window_size = (display_size,display_size)\n",
    "            env.cell_size = (\n",
    "                        env.window_size[0] // env.ncol,\n",
    "                        env.window_size[1] // env.nrow,\n",
    "                    )\n",
    "            env = gym.wrappers.RecordVideo(env, video_folder=\"video\")\n",
    "        \n",
    "        state, info = self.env.reset()\n",
    "        total_rewards = 0\n",
    "        \n",
    "        for e in range(n):\n",
    "            # print(state)\n",
    "\n",
    "            # 计算q_value\n",
    "            agent.q_table_update()\n",
    "\n",
    "            # 选择最优策略\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, terminated, truncted, info = env.step(action)\n",
    "\n",
    "            total_rewards+=reward\n",
    "            # 更新 R\n",
    "            # self.rewards[(state, action, new_state)] = reward\n",
    "            state = new_state\n",
    "            \n",
    "            # 更新 v\n",
    "            self.value_iteration()\n",
    "\n",
    "            if terminated:\n",
    "                state, info = self.env.reset()\n",
    "        return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95e7e251-2561-459b-8bd0-910b3ec8cece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV_NAME = \"FrozenLake-v1\"\n",
    "GAMMA = 0.8\n",
    "\n",
    "env = frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "env.spec = gym.spec(\"FrozenLake-v1\")\n",
    "# env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "# env = gym.wrappers.RecordVideo(env, video_folder=\"video\")\n",
    "\n",
    "agent = Agent(env)\n",
    "agent.episode(500, render=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
